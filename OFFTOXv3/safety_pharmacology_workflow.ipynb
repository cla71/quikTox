{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OFFTOXv3 — Safety Pharmacology & NHR Prediction Workflow\n\nEnd-to-end workflow for predicting compound activity against the **24-target safety pharmacology panel** using a 2-class binding scheme:\n\n| Class | Label | Definition |\n|-------|-------|------------|\n| 1 | **binding** | pChEMBL >= 5.0 (IC50/Ki < 10 uM) |\n| 0 | **non_binding** | pChEMBL < 5.0 (>= 10 uM) or confirmed inactive |\n\n**24-Target Panel:**\n- **Nuclear Hormone Receptors (14):** ERa, ER_beta, AR, GR, PR, MR, PPARg, PXR, CAR, LXRa, LXRb, FXR, RXRa, VDR\n- **Cardiac ion channels (3):** hERG, Cav1.2, Nav1.5\n- **CYP enzymes (5):** CYP3A4, CYP2D6, CYP2C9, CYP1A2, CYP2C19\n- **Transporters (2):** P-gp, BSEP\n\n**Data filters:** IC50 and Ki values with pChEMBL >= 4.0; at least 30 drug-like non-binders per target.\n\n---\n\n## How to use this notebook\n\n1. **Run cells 1-2** to set up and optionally refresh data from ChEMBL.\n2. **Run cells 3-9** to train and evaluate models on the training data.\n3. **Cell 10** evaluates on a held-out test set not used in training.\n4. **Cell 11** (Predict New Compounds) accepts any CSV with `compound_id`, `smiles`, and `target` columns.\n5. **Cell 12** generates the analysis report (Markdown).\n6. All visualizations render inline. Outputs are also saved to `outputs/`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Install dependencies (run once) ──────────────────────────────────\n# Uncomment the line below if running for the first time:\n# !pip install numpy<2 pandas scikit-learn xgboost lightgbm rdkit-pypi matplotlib seaborn scipy joblib requests\n\nimport json\nimport csv\nimport time\nimport warnings\nimport pickle\nimport hashlib\nimport requests\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom scipy import stats\n\nfrom rdkit import Chem\nfrom rdkit.Chem import Crippen, Descriptors, Lipinski, MolSurf, rdFingerprintGenerator\nfrom rdkit.Chem.Scaffolds import MurckoScaffold\n\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.metrics import (\n    average_precision_score,\n    confusion_matrix,\n    matthews_corrcoef,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n    classification_report,\n)\nfrom sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n%matplotlib inline\n\n# ── Paths ────────────────────────────────────────────────────────────\nNOTEBOOK_DIR = Path(\".\").resolve()\nDATA_PATH = NOTEBOOK_DIR / \"data\" / \"safety_targets_bioactivity.csv\"\nTEST_PATH = NOTEBOOK_DIR / \"data\" / \"test_compounds.csv\"\nOUTPUT_DIR = NOTEBOOK_DIR / \"outputs\"\nMODEL_DIR  = NOTEBOOK_DIR / \"model\"\nOUTPUT_DIR.mkdir(exist_ok=True)\nMODEL_DIR.mkdir(exist_ok=True)\n\n# ── 24-Target Safety Panel ───────────────────────────────────────────\nTARGET_PANEL = {\n    # Nuclear Hormone Receptors (14)\n    \"ERa\":     {\"chembl_id\": \"CHEMBL206\",  \"category\": \"Nuclear Hormone Receptor\"},\n    \"ER_beta\": {\"chembl_id\": \"CHEMBL242\",  \"category\": \"Nuclear Hormone Receptor\"},\n    \"AR\":      {\"chembl_id\": \"CHEMBL1871\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"GR\":      {\"chembl_id\": \"CHEMBL2034\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"PR\":      {\"chembl_id\": \"CHEMBL208\",  \"category\": \"Nuclear Hormone Receptor\"},\n    \"MR\":      {\"chembl_id\": \"CHEMBL1994\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"PPARg\":   {\"chembl_id\": \"CHEMBL235\",  \"category\": \"Nuclear Hormone Receptor\"},\n    \"PXR\":     {\"chembl_id\": \"CHEMBL3401\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"CAR\":     {\"chembl_id\": \"CHEMBL2248\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"LXRa\":    {\"chembl_id\": \"CHEMBL5231\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"LXRb\":    {\"chembl_id\": \"CHEMBL4309\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"FXR\":     {\"chembl_id\": \"CHEMBL2001\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"RXRa\":    {\"chembl_id\": \"CHEMBL2061\", \"category\": \"Nuclear Hormone Receptor\"},\n    \"VDR\":     {\"chembl_id\": \"CHEMBL1977\", \"category\": \"Nuclear Hormone Receptor\"},\n    # Cardiac Safety (3)\n    \"hERG\":    {\"chembl_id\": \"CHEMBL240\",  \"category\": \"Cardiac Safety\"},\n    \"Cav1.2\":  {\"chembl_id\": \"CHEMBL1940\", \"category\": \"Cardiac Safety\"},\n    \"Nav1.5\":  {\"chembl_id\": \"CHEMBL1993\", \"category\": \"Cardiac Safety\"},\n    # Hepatotoxicity / CYP (5)\n    \"CYP3A4\":  {\"chembl_id\": \"CHEMBL340\",  \"category\": \"Hepatotoxicity\"},\n    \"CYP2D6\":  {\"chembl_id\": \"CHEMBL289\",  \"category\": \"Hepatotoxicity\"},\n    \"CYP2C9\":  {\"chembl_id\": \"CHEMBL3397\", \"category\": \"Hepatotoxicity\"},\n    \"CYP1A2\":  {\"chembl_id\": \"CHEMBL3356\", \"category\": \"Hepatotoxicity\"},\n    \"CYP2C19\": {\"chembl_id\": \"CHEMBL3622\", \"category\": \"Hepatotoxicity\"},\n    # Transporters (2)\n    \"P-gp\":    {\"chembl_id\": \"CHEMBL4302\", \"category\": \"Transporter\"},\n    \"BSEP\":    {\"chembl_id\": \"CHEMBL4105\", \"category\": \"Transporter\"},\n}\n\n# ── Constants (2-class: binding vs non-binding) ──────────────────────\nRANDOM_STATE = 42\nACTIVITY_CLASS_MAP = {0: \"non_binding\", 1: \"binding\"}\nCLASS_COLORS = {0: \"#2ecc71\", 1: \"#e74c3c\"}\nNUM_CLASSES = 2\nCHEMBL_BASE_URL = \"https://www.ebi.ac.uk/chembl/api/data\"\n\nprint(f\"Data path  : {DATA_PATH}\")\nprint(f\"Test path  : {TEST_PATH}\")\nprint(f\"Output dir : {OUTPUT_DIR}\")\nprint(f\"Model dir  : {MODEL_DIR}\")\nprint(f\"Targets    : {len(TARGET_PANEL)} targets in panel\")\nprint(f\"Classes    : {NUM_CLASSES} ({', '.join(ACTIVITY_CLASS_MAP.values())})\")\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 2. ChEMBL Data Retrieval (24-Target Panel)\n\nPull IC50 and Ki bioactivity data from ChEMBL for all 24 targets.\n\n**Filters applied:**\n- Activity types: IC50, Ki only\n- Upper limit: < 100 uM (pChEMBL >= 4.0) to limit data volume and avoid pagination issues\n- At least 30 negatives per target (compounds confirmed inactive at > 100 uM)\n- Data quality: exact measurements only (`=` relation), valid SMILES required\n\n**Note:** If the ChEMBL API is unreachable, the notebook falls back to the pre-built CSV at `data/safety_targets_bioactivity.csv`. To regenerate from ChEMBL, set `REFRESH_FROM_CHEMBL = True` below.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── ChEMBL data retrieval for 24-target panel ────────────────────────\n# Set to True to pull fresh data from ChEMBL (requires internet access).\n# Set to False to use the pre-built CSV.\nREFRESH_FROM_CHEMBL = False\n\ndef fetch_chembl_activities(target_chembl_id, target_name, activity_types=(\"IC50\", \"Ki\"),\n                            pchembl_min=4.0, max_records=1000):\n    \"\"\"Fetch bioactivity data from ChEMBL REST API for one target.\n\n    Limits to IC50/Ki with pChEMBL >= 4.0 (< 100 uM) to control data volume.\n    Handles pagination with a hard cap to avoid runaway queries.\n    \"\"\"\n    all_records = []\n    for act_type in activity_types:\n        offset = 0\n        limit = 500  # conservative page size\n        while offset < max_records:\n            params = {\n                \"target_chembl_id\": target_chembl_id,\n                \"standard_type\": act_type,\n                \"pchembl_value__gte\": pchembl_min,\n                \"standard_relation\": \"=\",\n                \"limit\": limit,\n                \"offset\": offset,\n                \"format\": \"json\",\n            }\n            for attempt in range(4):\n                try:\n                    resp = requests.get(f\"{CHEMBL_BASE_URL}/activity.json\",\n                                        params=params, timeout=60)\n                    resp.raise_for_status()\n                    break\n                except Exception as e:\n                    if attempt < 3:\n                        time.sleep(2 ** (attempt + 1))\n                    else:\n                        print(f\"  FAILED {target_name}/{act_type} after 4 retries: {e}\")\n                        return all_records\n            data = resp.json()\n            activities = data.get(\"activities\", [])\n            if not activities:\n                break\n            for act in activities:\n                smi = act.get(\"canonical_smiles\")\n                pval = act.get(\"pchembl_value\")\n                if not smi or not pval:\n                    continue\n                all_records.append({\n                    \"molecule_chembl_id\": act.get(\"molecule_chembl_id\", \"\"),\n                    \"canonical_smiles\": smi,\n                    \"standard_type\": act.get(\"standard_type\", act_type),\n                    \"standard_relation\": \"=\",\n                    \"standard_value\": act.get(\"standard_value\", \"\"),\n                    \"standard_units\": act.get(\"standard_units\", \"nM\"),\n                    \"pchembl_value\": pval,\n                    \"activity_comment\": act.get(\"activity_comment\", \"\"),\n                    \"assay_chembl_id\": act.get(\"assay_chembl_id\", \"\"),\n                    \"assay_type\": act.get(\"assay_type\", \"\"),\n                    \"target_chembl_id\": target_chembl_id,\n                    \"target_pref_name\": act.get(\"target_pref_name\", \"\"),\n                    \"document_chembl_id\": act.get(\"document_chembl_id\", \"\"),\n                    \"src_id\": act.get(\"src_id\", \"\"),\n                    \"data_validity_comment\": act.get(\"data_validity_comment\", \"\"),\n                    \"safety_category\": TARGET_PANEL[target_name][\"category\"],\n                    \"target_common_name\": target_name,\n                })\n            if len(activities) < limit:\n                break\n            offset += limit\n            time.sleep(0.5)  # rate limiting\n        print(f\"  {target_name}/{act_type}: {len([r for r in all_records if r['standard_type']==act_type])} records\")\n    return all_records\n\n\ndef fetch_chembl_inactives(target_chembl_id, target_name, min_inactive=30, max_records=2000):\n    \"\"\"Fetch confirmed inactive compounds (right-censored > at >= 100 uM).\"\"\"\n    inactive = []\n    offset = 0\n    limit = 500\n    while offset < max_records and len(inactive) < min_inactive * 3:\n        params = {\n            \"target_chembl_id\": target_chembl_id,\n            \"standard_relation\": \">\",\n            \"standard_type__in\": \"IC50,Ki\",\n            \"limit\": limit,\n            \"offset\": offset,\n            \"format\": \"json\",\n        }\n        for attempt in range(4):\n            try:\n                resp = requests.get(f\"{CHEMBL_BASE_URL}/activity.json\",\n                                    params=params, timeout=60)\n                resp.raise_for_status()\n                break\n            except Exception:\n                if attempt < 3:\n                    time.sleep(2 ** (attempt + 1))\n                else:\n                    return inactive\n        data = resp.json()\n        activities = data.get(\"activities\", [])\n        if not activities:\n            break\n        for act in activities:\n            smi = act.get(\"canonical_smiles\")\n            val = act.get(\"standard_value\")\n            if not smi:\n                continue\n            try:\n                if float(val) >= 100000:  # >= 100 uM in nM\n                    inactive.append({\n                        \"molecule_chembl_id\": act.get(\"molecule_chembl_id\", \"\"),\n                        \"canonical_smiles\": smi,\n                        \"standard_type\": act.get(\"standard_type\", \"\"),\n                        \"standard_relation\": \">\",\n                        \"standard_value\": val,\n                        \"standard_units\": \"nM\",\n                        \"pchembl_value\": \"\",\n                        \"activity_comment\": \"Confirmed inactive (> 100 uM)\",\n                        \"assay_chembl_id\": act.get(\"assay_chembl_id\", \"\"),\n                        \"assay_type\": act.get(\"assay_type\", \"\"),\n                        \"target_chembl_id\": target_chembl_id,\n                        \"target_pref_name\": act.get(\"target_pref_name\", \"\"),\n                        \"document_chembl_id\": act.get(\"document_chembl_id\", \"\"),\n                        \"src_id\": act.get(\"src_id\", \"\"),\n                        \"data_validity_comment\": \"\",\n                        \"safety_category\": TARGET_PANEL[target_name][\"category\"],\n                        \"target_common_name\": target_name,\n                        \"activity_class\": \"0\",\n                        \"activity_class_label\": \"non_binding\",\n                    })\n            except (ValueError, TypeError):\n                continue\n        if len(activities) < limit:\n            break\n        offset += limit\n        time.sleep(0.5)\n\n    # Deduplicate by molecule\n    seen = set()\n    deduped = []\n    for r in inactive:\n        mid = r[\"molecule_chembl_id\"]\n        if mid not in seen:\n            seen.add(mid)\n            deduped.append(r)\n    return deduped[:min_inactive + 10]\n\n\nif REFRESH_FROM_CHEMBL:\n    print(\"Pulling data from ChEMBL for 24 targets...\")\n    all_chembl_rows = []\n    for tname, tinfo in TARGET_PANEL.items():\n        print(f\"\\n--- {tname} ({tinfo['chembl_id']}) ---\")\n        active = fetch_chembl_activities(tinfo[\"chembl_id\"], tname)\n        # Assign 2-class activity labels\n        for r in active:\n            try:\n                p = float(r[\"pchembl_value\"])\n                r[\"activity_class\"] = \"1\" if p >= 5.0 else \"0\"\n                r[\"activity_class_label\"] = \"binding\" if p >= 5.0 else \"non_binding\"\n            except (ValueError, TypeError):\n                continue\n        all_chembl_rows.extend(active)\n\n        inact = fetch_chembl_inactives(tinfo[\"chembl_id\"], tname, min_inactive=30)\n        all_chembl_rows.extend(inact)\n        print(f\"  Total: {len(active)} active + {len(inact)} inactive\")\n\n    # Save\n    fieldnames = [\n        \"molecule_chembl_id\", \"canonical_smiles\", \"standard_type\",\n        \"standard_relation\", \"standard_value\", \"standard_units\",\n        \"pchembl_value\", \"activity_comment\", \"assay_chembl_id\",\n        \"assay_type\", \"target_chembl_id\", \"target_pref_name\",\n        \"document_chembl_id\", \"src_id\", \"data_validity_comment\",\n        \"safety_category\", \"target_common_name\", \"activity_class\",\n        \"activity_class_label\",\n    ]\n    with DATA_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        for row in all_chembl_rows:\n            writer.writerow({k: row.get(k, \"\") for k in fieldnames})\n    print(f\"\\nSaved {len(all_chembl_rows)} records to {DATA_PATH}\")\nelse:\n    print(f\"Using pre-built dataset at {DATA_PATH}\")\n    print(\"Set REFRESH_FROM_CHEMBL = True to pull fresh data from ChEMBL.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Data Loading & Exploration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Core data structures ─────────────────────────────────────────────\n@dataclass\nclass SplitData:\n    train_idx: np.ndarray\n    val_idx: np.ndarray\n    test_idx: np.ndarray\n\n\ndef load_and_clean_data(path: Path) -> List[dict]:\n    \"\"\"Load training CSV and assign 2-class activity labels.\n\n    Classes:\n        1 - binding:     pChEMBL >= 5.0  (< 10 uM)\n        0 - non_binding: pChEMBL < 5.0  (>= 10 uM) or confirmed inactive\n    \"\"\"\n    rows = []\n    with path.open(newline=\"\", encoding=\"utf-8\") as fh:\n        reader = csv.DictReader(fh)\n        for row in reader:\n            smi = row.get(\"canonical_smiles\")\n            if not smi:\n                continue\n\n            raw_class = row.get(\"activity_class\", \"\")\n            label = row.get(\"activity_class_label\", \"\")\n            if raw_class == \"0\" or label in (\"inactive\", \"non_binding\"):\n                row[\"pchembl_value\"] = None\n                row[\"activity_class\"] = 0\n                rows.append(row)\n                continue\n\n            if row.get(\"standard_relation\") != \"=\":\n                continue\n            if not row.get(\"pchembl_value\"):\n                continue\n            try:\n                pchembl = float(row[\"pchembl_value\"])\n            except ValueError:\n                continue\n            if pchembl < 4.0:\n                continue\n            row[\"pchembl_value\"] = pchembl\n            row[\"activity_class\"] = 1 if pchembl >= 5.0 else 0\n            rows.append(row)\n\n    deduped: dict = {}\n    for row in rows:\n        key = (row.get(\"molecule_chembl_id\"), row.get(\"target_chembl_id\"))\n        existing = deduped.get(key)\n        if existing is None:\n            deduped[key] = row\n        else:\n            existing_p = existing.get(\"pchembl_value\")\n            current_p = row.get(\"pchembl_value\")\n            if current_p is not None and (existing_p is None or current_p > existing_p):\n                deduped[key] = row\n    return list(deduped.values())\n\n\n# ── Load ──────────────────────────────────────────────────────────────\ndata = load_and_clean_data(DATA_PATH)\nlabels_all = np.array([row[\"activity_class\"] for row in data], dtype=int)\ntargets_all = [row.get(\"target_common_name\", \"unknown\") for row in data]\n\nprint(f\"Loaded {len(data)} compound-target records\")\nprint(f\"Targets: {sorted(set(targets_all))}\")\nprint(f\"\\nClass distribution:\")\nfor cls in sorted(ACTIVITY_CLASS_MAP):\n    n = int((labels_all == cls).sum())\n    print(f\"  {cls} ({ACTIVITY_CLASS_MAP[cls]:>12s}): {n:>5d}  ({100*n/len(data):.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Exploratory visualizations ────────────────────────────────────────\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 1. Class distribution\nclass_counts = Counter(labels_all)\nbars = axes[0].bar(\n    [ACTIVITY_CLASS_MAP[c] for c in sorted(class_counts)],\n    [class_counts[c] for c in sorted(class_counts)],\n    color=[CLASS_COLORS[c] for c in sorted(class_counts)],\n    edgecolor=\"black\",\n)\nfor bar, c in zip(bars, sorted(class_counts)):\n    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10,\n                 str(class_counts[c]), ha=\"center\", fontweight=\"bold\")\naxes[0].set_title(\"Class Distribution\")\naxes[0].set_ylabel(\"Count\")\n\n# 2. Per-target breakdown\ntarget_class_df = pd.DataFrame({\"target\": targets_all, \"class\": labels_all})\ntarget_order = sorted(set(targets_all))\nclass_by_target = target_class_df.groupby([\"target\", \"class\"]).size().unstack(fill_value=0)\nclass_by_target = class_by_target.reindex(columns=list(range(NUM_CLASSES)), fill_value=0)\nclass_by_target.columns = [ACTIVITY_CLASS_MAP[c] for c in class_by_target.columns]\nclass_by_target.loc[target_order].plot.barh(\n    stacked=True, ax=axes[1],\n    color=[CLASS_COLORS[c] for c in range(NUM_CLASSES)],\n    edgecolor=\"black\",\n)\naxes[1].set_title(\"Compounds per Target\")\naxes[1].set_xlabel(\"Count\")\naxes[1].legend(title=\"Class\", loc=\"lower right\")\n\n# 3. pChEMBL distribution (active compounds only)\npchembl_vals = [float(row[\"pchembl_value\"]) for row in data if row[\"pchembl_value\"] is not None]\naxes[2].hist(pchembl_vals, bins=30, color=\"#3498db\", edgecolor=\"black\", alpha=0.8)\naxes[2].axvline(5.0, color=\"red\", ls=\"--\", lw=2, label=\"Binding threshold (5.0)\")\naxes[2].set_title(\"pChEMBL Value Distribution\")\naxes[2].set_xlabel(\"pChEMBL\")\naxes[2].set_ylabel(\"Count\")\naxes[2].legend()\n\nfig.tight_layout()\nfig.savefig(OUTPUT_DIR / \"01_data_exploration.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(\"Saved: outputs/01_data_exploration.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Feature Engineering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors(smiles: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Compute 10 physicochemical descriptors per molecule.\"\"\"\n",
    "    descriptor_functions = {\n",
    "        \"MW\": Descriptors.MolWt,\n",
    "        \"LogP\": Crippen.MolLogP,\n",
    "        \"HBA\": Lipinski.NumHAcceptors,\n",
    "        \"HBD\": Lipinski.NumHDonors,\n",
    "        \"TPSA\": MolSurf.TPSA,\n",
    "        \"RotatableBonds\": Lipinski.NumRotatableBonds,\n",
    "        \"AromaticRings\": Lipinski.NumAromaticRings,\n",
    "        \"HeavyAtoms\": Lipinski.HeavyAtomCount,\n",
    "        \"FractionCSP3\": Lipinski.FractionCSP3,\n",
    "        \"MolMR\": Crippen.MolMR,\n",
    "    }\n",
    "    rows = []\n",
    "    for smi in smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            rows.append([np.nan] * len(descriptor_functions))\n",
    "            continue\n",
    "        rows.append([func(mol) for func in descriptor_functions.values()])\n",
    "    return np.array(rows, dtype=float), list(descriptor_functions.keys())\n",
    "\n",
    "\n",
    "def compute_morgan_fingerprints(smiles: List[str], n_bits: int = 2048) -> np.ndarray:\n",
    "    \"\"\"Compute 2048-bit Morgan fingerprints (ECFP4, radius=2).\"\"\"\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=n_bits)\n",
    "    fps = []\n",
    "    for smi in smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            fps.append(np.zeros(n_bits, dtype=int))\n",
    "            continue\n",
    "        fps.append(np.array(gen.GetFingerprint(mol)))\n",
    "    return np.array(fps)\n",
    "\n",
    "\n",
    "def build_feature_matrix(\n",
    "    rows: List[dict],\n",
    "    selected_columns: Optional[List[str]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Build combined feature matrix: descriptors + fingerprints + target encoding.\n",
    "\n",
    "    When selected_columns is None (training), variance filtering is applied and\n",
    "    the surviving column names are returned.  When selected_columns is provided\n",
    "    (prediction), the matrix is aligned to those columns.\n",
    "    \"\"\"\n",
    "    smiles = [row[\"canonical_smiles\"] for row in rows]\n",
    "    targets = [row.get(\"target_common_name\", row.get(\"target\", \"\")) for row in rows]\n",
    "    labels = np.array([row.get(\"activity_class\", -1) for row in rows], dtype=int)\n",
    "\n",
    "    descriptors, desc_names = compute_descriptors(smiles)\n",
    "    fingerprints = compute_morgan_fingerprints(smiles)\n",
    "    fp_names = [f\"FP_{i}\" for i in range(fingerprints.shape[1])]\n",
    "\n",
    "    target_names = sorted({t for t in targets if t})\n",
    "    target_map = {name: idx for idx, name in enumerate(target_names)}\n",
    "    target_matrix = np.zeros((len(rows), len(target_names)), dtype=float)\n",
    "    for idx, target in enumerate(targets):\n",
    "        if target in target_map:\n",
    "            target_matrix[idx, target_map[target]] = 1.0\n",
    "\n",
    "    feature_matrix = np.concatenate([descriptors, fingerprints, target_matrix], axis=1)\n",
    "    columns = desc_names + fp_names + [f\"target_{n}\" for n in target_names]\n",
    "\n",
    "    if selected_columns is None:\n",
    "        variances = np.nanvar(feature_matrix, axis=0)\n",
    "        mask = variances > 0.01\n",
    "        feature_matrix = np.nan_to_num(feature_matrix[:, mask], nan=0.0)\n",
    "        selected_columns = [col for col, keep in zip(columns, mask) if keep]\n",
    "    else:\n",
    "        col_index = {col: idx for idx, col in enumerate(columns)}\n",
    "        aligned = np.zeros((len(rows), len(selected_columns)), dtype=float)\n",
    "        for out_idx, col in enumerate(selected_columns):\n",
    "            if col in col_index:\n",
    "                aligned[:, out_idx] = np.nan_to_num(\n",
    "                    feature_matrix[:, col_index[col]], nan=0.0\n",
    "                )\n",
    "        feature_matrix = aligned\n",
    "\n",
    "    return feature_matrix, labels, selected_columns\n",
    "\n",
    "\n",
    "# ── Build features ────────────────────────────────────────────────────\n",
    "print(\"Computing features (descriptors + 2048-bit Morgan FP + target encoding)...\")\n",
    "t0 = time.time()\n",
    "features, labels, selected_columns = build_feature_matrix(data)\n",
    "print(f\"  Done in {time.time() - t0:.1f}s\")\n",
    "print(f\"  Feature matrix shape: {features.shape}\")\n",
    "print(f\"  Features retained after variance filter: {len(selected_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Scaffold Split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split(smiles: List[str], y: np.ndarray, random_state: int = 42) -> SplitData:\n",
    "    \"\"\"Split data by Murcko scaffold to avoid data leakage (60/20/20).\"\"\"\n",
    "    scaffolds: Dict[str, List[int]] = {}\n",
    "    for idx, smi in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            scaffold = \"\"\n",
    "        else:\n",
    "            scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol)\n",
    "        scaffolds.setdefault(scaffold, []).append(idx)\n",
    "\n",
    "    scaffold_sets = sorted(scaffolds.values(), key=len, reverse=True)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(scaffold_sets)\n",
    "\n",
    "    n_total = len(smiles)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for group in scaffold_sets:\n",
    "        if len(train_idx) + len(group) <= n_train:\n",
    "            train_idx.extend(group)\n",
    "        elif len(val_idx) + len(group) <= n_val:\n",
    "            val_idx.extend(group)\n",
    "        else:\n",
    "            test_idx.extend(group)\n",
    "\n",
    "    return SplitData(\n",
    "        train_idx=np.array(train_idx),\n",
    "        val_idx=np.array(val_idx),\n",
    "        test_idx=np.array(test_idx),\n",
    "    )\n",
    "\n",
    "\n",
    "split = scaffold_split([row[\"canonical_smiles\"] for row in data], labels, RANDOM_STATE)\n",
    "\n",
    "X_train, y_train = features[split.train_idx], labels[split.train_idx]\n",
    "X_val, y_val     = features[split.val_idx],   labels[split.val_idx]\n",
    "X_test, y_test   = features[split.test_idx],  labels[split.test_idx]\n",
    "\n",
    "print(f\"Train : {len(X_train):>5d}  |  Val : {len(X_val):>5d}  |  Test : {len(X_test):>5d}\")\n",
    "for name, y_sub in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    counts = Counter(y_sub)\n",
    "    parts = \", \".join(f\"{ACTIVITY_CLASS_MAP[c]}={counts.get(c, 0)}\" for c in range(NUM_CLASSES))\n",
    "    print(f\"  {name:>5s}: {parts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Model Training & Cross-Validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ece_score_fn(y_true, y_prob, n_bins=10):\n    \"\"\"Expected Calibration Error and Maximum Calibration Error.\"\"\"\n    bins = np.linspace(0, 1, n_bins + 1)\n    binids = np.digitize(y_prob, bins) - 1\n    ece, mce = 0.0, 0.0\n    for i in range(n_bins):\n        mask = binids == i\n        if not np.any(mask):\n            continue\n        avg_conf = y_prob[mask].mean()\n        avg_acc = y_true[mask].mean()\n        gap = abs(avg_conf - avg_acc)\n        ece += gap * mask.mean()\n        mce = max(mce, gap)\n    return ece, mce\n\n\ndef get_models(random_state: int) -> Dict[str, Tuple[Pipeline, Dict[str, list]]]:\n    \"\"\"Return three model pipelines with hyperparameter search spaces.\"\"\"\n    return {\n        \"RandomForest\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", RandomForestClassifier(random_state=random_state, n_jobs=-1)),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [10, 20, None],\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__max_features\": [\"sqrt\", \"log2\", 0.3],\n                \"model__class_weight\": [\"balanced\", None],\n            },\n        ),\n        \"XGBoost\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", XGBClassifier(\n                    random_state=random_state,\n                    objective=\"binary:logistic\",\n                    eval_metric=\"logloss\",\n                    n_jobs=-1,\n                    verbosity=0,\n                )),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [3, 5, 7],\n                \"model__learning_rate\": [0.01, 0.05, 0.1],\n                \"model__subsample\": [0.6, 0.8, 1.0],\n                \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n            },\n        ),\n        \"LightGBM\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", LGBMClassifier(\n                    random_state=random_state, n_jobs=-1, verbose=-1,\n                    objective=\"binary\",\n                )),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [-1, 5, 10],\n                \"model__learning_rate\": [0.01, 0.05, 0.1],\n                \"model__num_leaves\": [31, 63, 127],\n                \"model__subsample\": [0.6, 0.8, 1.0],\n            },\n        ),\n    }\n\n\n# ── Train all models ──────────────────────────────────────────────────\nmodels = get_models(RANDOM_STATE)\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=RANDOM_STATE)\n\ncv_summary = []\nbest_estimators = {}\ncalibration_metrics = {}\nfold_scores: Dict[str, List[float]] = {}\ntrain_times = {}\n\nfor name, (pipeline, param_grid) in models.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {name}\")\n    print(f\"{'='*60}\")\n\n    # Hyperparameter search\n    search = RandomizedSearchCV(\n        pipeline,\n        param_distributions=param_grid,\n        n_iter=5,\n        scoring=\"roc_auc\",\n        cv=3,\n        random_state=RANDOM_STATE,\n        n_jobs=1,  # avoid over-subscription with model-level n_jobs=-1\n    )\n    t0 = time.time()\n    search.fit(X_train, y_train)\n    train_times[name] = time.time() - t0\n    best_estimators[name] = search.best_estimator_\n    print(f\"  Best params: {search.best_params_}\")\n    print(f\"  Train time : {train_times[name]:.1f}s\")\n\n    # Cross-validation evaluation\n    scores, pr_scores, mcc_scores = [], [], []\n    for train_idx, test_idx in cv.split(X_train, y_train):\n        X_tr, X_te = X_train[train_idx], X_train[test_idx]\n        y_tr, y_te = y_train[train_idx], y_train[test_idx]\n        est = search.best_estimator_\n        est.fit(X_tr, y_tr)\n        probs = est.predict_proba(X_te)\n        preds = est.predict(X_te)\n        scores.append(roc_auc_score(y_te, probs[:, 1]))\n        pr_scores.append(average_precision_score(y_te, probs[:, 1]))\n        mcc_scores.append(matthews_corrcoef(y_te, preds))\n\n    cv_summary.append({\n        \"model\": name,\n        \"roc_auc_mean\": np.mean(scores), \"roc_auc_std\": np.std(scores),\n        \"pr_auc_mean\": np.mean(pr_scores), \"pr_auc_std\": np.std(pr_scores),\n        \"mcc_mean\": np.mean(mcc_scores), \"mcc_std\": np.std(mcc_scores),\n    })\n    fold_scores[name] = scores\n\n    # Validation calibration\n    val_probs = search.best_estimator_.predict_proba(X_val)\n    val_probs_true = val_probs[np.arange(len(y_val)), y_val]\n    ece_val, _ = ece_score_fn(np.ones(len(y_val)), val_probs_true)\n    calibration_metrics[name] = ece_val\n\n    print(f\"  CV ROC-AUC : {np.mean(scores):.4f} +/- {np.std(scores):.4f}\")\n    print(f\"  CV PR-AUC  : {np.mean(pr_scores):.4f} +/- {np.std(pr_scores):.4f}\")\n    print(f\"  CV MCC     : {np.mean(mcc_scores):.4f} +/- {np.std(mcc_scores):.4f}\")\n\nprint(f\"\\nTraining complete. {len(models)} models evaluated.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Model Evaluation & Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Select best model & refit on train+val ────────────────────────────\ncv_summary_sorted = sorted(cv_summary, key=lambda r: r[\"roc_auc_mean\"], reverse=True)\nbest_model_name = cv_summary_sorted[0][\"model\"]\nbest_model = best_estimators[best_model_name]\nbest_model.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n\nprint(\"Cross-validation summary (sorted by ROC-AUC):\")\nprint(\"-\" * 75)\nprint(f\"{'Model':<15s} {'ROC-AUC':>18s} {'PR-AUC':>18s} {'MCC':>18s}\")\nprint(\"-\" * 75)\nfor row in cv_summary_sorted:\n    print(f\"{row['model']:<15s} \"\n          f\"{row['roc_auc_mean']:.4f} +/- {row['roc_auc_std']:.4f}  \"\n          f\"{row['pr_auc_mean']:.4f} +/- {row['pr_auc_std']:.4f}  \"\n          f\"{row['mcc_mean']:.4f} +/- {row['mcc_std']:.4f}\")\nprint(\"-\" * 75)\nprint(f\"Best model: {best_model_name}\")\n\n# ── Test-set evaluation ───────────────────────────────────────────────\ntest_probs = best_model.predict_proba(X_test)\ntest_preds = best_model.predict(X_test)\ntest_roc = roc_auc_score(y_test, test_probs[:, 1])\ntest_pr = average_precision_score(y_test, test_probs[:, 1])\ntest_mcc = matthews_corrcoef(y_test, test_preds)\n\n# Calibration\ncalibrated = CalibratedClassifierCV(best_model, method=\"isotonic\", cv=3)\ncalibrated.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\ncal_probs = calibrated.predict_proba(X_test)\ncal_probs_true = cal_probs[np.arange(len(y_test)), y_test]\nece, mce = ece_score_fn(np.ones(len(y_test)), cal_probs_true)\n\nprint(f\"\\nTest Set Metrics ({best_model_name}):\")\nprint(f\"  ROC-AUC         : {test_roc:.4f}\")\nprint(f\"  PR-AUC          : {test_pr:.4f}\")\nprint(f\"  MCC             : {test_mcc:.4f}\")\nprint(f\"  ECE (calibrated): {ece:.4f}\")\nprint(f\"  MCE (calibrated): {mce:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(\n    y_test, test_preds,\n    target_names=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n    digits=3,\n))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Figure 2: ROC Curves (per-class) ──────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor cls in range(NUM_CLASSES):\n    label = ACTIVITY_CLASS_MAP[cls]\n    binary_true = (y_test == cls).astype(int)\n    if binary_true.sum() == 0:\n        axes[cls].set_title(f\"ROC — {label} (no samples)\")\n        continue\n    fpr, tpr, _ = roc_curve(binary_true, test_probs[:, cls])\n    auc_val = roc_auc_score(binary_true, test_probs[:, cls])\n    axes[cls].plot(fpr, tpr, color=CLASS_COLORS[cls], lw=2,\n                   label=f\"AUC = {auc_val:.3f}\")\n    axes[cls].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5)\n    axes[cls].set_xlabel(\"False Positive Rate\")\n    axes[cls].set_ylabel(\"True Positive Rate\")\n    axes[cls].set_title(f\"ROC — {label}\")\n    axes[cls].legend(loc=\"lower right\", fontsize=12)\n    axes[cls].set_xlim([-0.02, 1.02])\n    axes[cls].set_ylim([-0.02, 1.02])\n\nfig.suptitle(f\"Per-Class ROC Curves ({best_model_name})\", fontsize=14, y=1.02)\nfig.tight_layout()\nfig.savefig(OUTPUT_DIR / \"02_roc_curves.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Figure 3: Precision-Recall Curves ────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor cls in range(NUM_CLASSES):\n    label = ACTIVITY_CLASS_MAP[cls]\n    binary_true = (y_test == cls).astype(int)\n    if binary_true.sum() == 0:\n        axes[cls].set_title(f\"PR — {label} (no samples)\")\n        continue\n    precision, recall, _ = precision_recall_curve(binary_true, test_probs[:, cls])\n    ap = average_precision_score(binary_true, test_probs[:, cls])\n    axes[cls].plot(recall, precision, color=CLASS_COLORS[cls], lw=2,\n                   label=f\"AP = {ap:.3f}\")\n    baseline = binary_true.mean()\n    axes[cls].axhline(baseline, color=\"gray\", ls=\"--\", lw=1, alpha=0.5,\n                      label=f\"Baseline = {baseline:.3f}\")\n    axes[cls].set_xlabel(\"Recall\")\n    axes[cls].set_ylabel(\"Precision\")\n    axes[cls].set_title(f\"PR — {label}\")\n    axes[cls].legend(loc=\"upper right\", fontsize=11)\n    axes[cls].set_xlim([-0.02, 1.02])\n    axes[cls].set_ylim([-0.02, 1.02])\n\nfig.suptitle(f\"Per-Class Precision-Recall Curves ({best_model_name})\", fontsize=14, y=1.02)\nfig.tight_layout()\nfig.savefig(OUTPUT_DIR / \"03_pr_curves.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 4: Confusion Matrix ───────────────────────────────────────\n",
    "cm = confusion_matrix(y_test, test_preds, labels=list(range(NUM_CLASSES)))\n",
    "cm_pct = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0],\n",
    "            xticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n",
    "            yticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)])\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "axes[0].set_title(\"Confusion Matrix (counts)\")\n",
    "\n",
    "# Percentages\n",
    "sns.heatmap(cm_pct, annot=True, fmt=\".1f\", cmap=\"Blues\", ax=axes[1],\n",
    "            xticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n",
    "            yticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)])\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "axes[1].set_title(\"Confusion Matrix (% per row)\")\n",
    "\n",
    "fig.suptitle(f\"Test Set Confusion Matrix ({best_model_name})\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"04_confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Figure 5: Calibration Curves ─────────────────────────────────────\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nfor cls in range(NUM_CLASSES):\n    label = ACTIVITY_CLASS_MAP[cls]\n    binary_true = (y_test == cls).astype(int)\n    cls_cal_probs = cal_probs[:, cls]\n    if binary_true.sum() == 0:\n        axes[cls].set_title(f\"Calibration — {label} (no samples)\")\n        continue\n    prob_true, prob_pred = calibration_curve(binary_true, cls_cal_probs, n_bins=10)\n    axes[cls].plot(prob_pred, prob_true, \"o-\", color=CLASS_COLORS[cls], lw=2,\n                   label=f\"{label}\")\n    axes[cls].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5, label=\"Perfectly calibrated\")\n    axes[cls].set_xlabel(\"Mean Predicted Probability\")\n    axes[cls].set_ylabel(\"Fraction of Positives\")\n    axes[cls].set_title(f\"Calibration — {label}\")\n    axes[cls].legend(loc=\"upper left\", fontsize=11)\n    axes[cls].set_xlim([-0.02, 1.02])\n    axes[cls].set_ylim([-0.02, 1.02])\n\nfig.suptitle(f\"Calibration Curves (isotonic, {best_model_name})\", fontsize=14, y=1.02)\nfig.tight_layout()\nfig.savefig(OUTPUT_DIR / \"05_calibration_curves.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(f\"ECE = {ece:.4f}, MCE = {mce:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 6: Feature Importance (top 20) ────────────────────────────\n",
    "if hasattr(best_model.named_steps[\"model\"], \"feature_importances_\"):\n",
    "    importances = best_model.named_steps[\"model\"].feature_importances_\n",
    "    top_k = 20\n",
    "    indices = np.argsort(importances)[-top_k:]\n",
    "    top_features = [selected_columns[i] for i in indices]\n",
    "    top_values = importances[indices]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.barh(range(top_k), top_values, color=\"#3498db\", edgecolor=\"black\")\n",
    "    ax.set_yticks(range(top_k))\n",
    "    ax.set_yticklabels(top_features)\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {top_k} Feature Importances ({best_model_name})\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUTPUT_DIR / \"06_feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Statistical Comparison & MCDA"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paired t-tests with Bonferroni correction ────────────────────────\n",
    "model_names = [row[\"model\"] for row in cv_summary_sorted]\n",
    "stat_rows = []\n",
    "for i, model_a in enumerate(model_names):\n",
    "    for model_b in model_names[i + 1:]:\n",
    "        sa = np.array(fold_scores.get(model_a, []))\n",
    "        sb = np.array(fold_scores.get(model_b, []))\n",
    "        if len(sa) == 0 or len(sb) == 0:\n",
    "            continue\n",
    "        t_stat, p_val = stats.ttest_rel(sa, sb)\n",
    "        pooled = np.std(np.concatenate([sa, sb]))\n",
    "        cohen_d = (sa.mean() - sb.mean()) / pooled if pooled else 0.0\n",
    "        stat_rows.append({\n",
    "            \"Model A\": model_a, \"Model B\": model_b,\n",
    "            \"t-stat\": t_stat, \"p-value\": p_val, \"Cohen's d\": cohen_d,\n",
    "        })\n",
    "\n",
    "if stat_rows:\n",
    "    bonferroni = 0.05 / len(stat_rows)\n",
    "    for r in stat_rows:\n",
    "        r[\"Significant\"] = \"Yes\" if r[\"p-value\"] < bonferroni else \"No\"\n",
    "        r[\"Bonferroni alpha\"] = bonferroni\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows)\n",
    "print(\"Statistical Comparison (paired t-test on CV ROC-AUC folds):\")\n",
    "print(f\"Bonferroni-corrected alpha = {bonferroni:.4f}\")\n",
    "display(stat_df.style.format({\n",
    "    \"t-stat\": \"{:.4f}\", \"p-value\": \"{:.6f}\",\n",
    "    \"Cohen's d\": \"{:.4f}\", \"Bonferroni alpha\": \"{:.4f}\",\n",
    "}).set_caption(\"Pairwise Model Comparison\"))\n",
    "\n",
    "# ── MCDA Ranking ─────────────────────────────────────────────────────\n",
    "mcda_rows = []\n",
    "for row in cv_summary_sorted:\n",
    "    name = row[\"model\"]\n",
    "    mcda_rows.append({\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": row[\"roc_auc_mean\"],\n",
    "        \"pr_auc\": row[\"pr_auc_mean\"],\n",
    "        \"calibration\": max(0.0, 1 - calibration_metrics.get(name, ece)),\n",
    "        \"robustness\": max(0.0, 1 - row[\"roc_auc_std\"]),\n",
    "        \"efficiency\": 1.0 / (1.0 + train_times.get(name, 1.0)),\n",
    "        \"interpretability\": 1.0 if name in {\"RandomForest\", \"LightGBM\", \"XGBoost\"} else 0.5,\n",
    "    })\n",
    "\n",
    "weights = {\n",
    "    \"roc_auc\": 0.25, \"pr_auc\": 0.20, \"calibration\": 0.20,\n",
    "    \"robustness\": 0.15, \"efficiency\": 0.10, \"interpretability\": 0.10,\n",
    "}\n",
    "for metric in weights:\n",
    "    vals = [r[metric] for r in mcda_rows]\n",
    "    mn, mx = min(vals), max(vals)\n",
    "    for r in mcda_rows:\n",
    "        r[metric] = (r[metric] - mn) / (mx - mn) if mx > mn else 1.0\n",
    "for r in mcda_rows:\n",
    "    r[\"composite\"] = sum(r[m] * w for m, w in weights.items())\n",
    "mcda_rows = sorted(mcda_rows, key=lambda r: r[\"composite\"], reverse=True)\n",
    "\n",
    "print(\"\\nMCDA Ranking:\")\n",
    "mcda_df = pd.DataFrame(mcda_rows)\n",
    "display(mcda_df.style.format(\"{:.4f}\", subset=mcda_df.columns[1:]).set_caption(\n",
    "    \"Multi-Criteria Decision Analysis\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 9. Uncertainty Quantification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Conformal Prediction ──────────────────────────────────────────────\n",
    "def conformal_prediction(probs, y_true, alpha=0.05):\n",
    "    scores = 1.0 - probs[np.arange(len(y_true)), y_true]\n",
    "    q = np.quantile(scores, 1 - alpha, method=\"higher\")\n",
    "    prediction_sets = probs >= (1.0 - q)\n",
    "    coverage = prediction_sets[np.arange(len(y_true)), y_true].mean()\n",
    "    return prediction_sets, coverage, q\n",
    "\n",
    "pred_sets, coverage, q_threshold = conformal_prediction(cal_probs, y_test)\n",
    "set_sizes = pred_sets.sum(axis=1)\n",
    "\n",
    "# ── Applicability Domain (k-NN distance) ─────────────────────────────\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_train)\n",
    "train_dists = nn.kneighbors(X_train)[0].mean(axis=1)\n",
    "ad_threshold = np.percentile(train_dists, 95)\n",
    "test_dists = nn.kneighbors(X_test)[0].mean(axis=1)\n",
    "ood_rate = (test_dists > ad_threshold).mean()\n",
    "\n",
    "print(f\"Conformal Prediction (alpha=0.05):\")\n",
    "print(f\"  Coverage          : {coverage:.4f}  (target: 0.95)\")\n",
    "print(f\"  Avg set size      : {set_sizes.mean():.2f}\")\n",
    "print(f\"  Quantile threshold: {q_threshold:.4f}\")\n",
    "print(f\"\\nApplicability Domain:\")\n",
    "print(f\"  AD threshold (95th pct): {ad_threshold:.4f}\")\n",
    "print(f\"  Out-of-domain rate     : {ood_rate:.2%}\")\n",
    "\n",
    "# ── Figure 7: Uncertainty plots ──────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Set size distribution\n",
    "unique_sizes, counts = np.unique(set_sizes, return_counts=True)\n",
    "axes[0].bar(unique_sizes.astype(str), counts, color=\"#9b59b6\", edgecolor=\"black\")\n",
    "for s, c in zip(unique_sizes, counts):\n",
    "    axes[0].text(str(s), c + 2, str(c), ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Prediction Set Size\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(f\"Conformal Set Sizes (coverage={coverage:.2%})\")\n",
    "\n",
    "# Distance to training set\n",
    "axes[1].hist(test_dists, bins=30, color=\"#1abc9c\", edgecolor=\"black\", alpha=0.8,\n",
    "             label=\"Test compounds\")\n",
    "axes[1].axvline(ad_threshold, color=\"red\", ls=\"--\", lw=2,\n",
    "                label=f\"AD threshold ({ad_threshold:.2f})\")\n",
    "axes[1].set_xlabel(\"Mean k-NN Distance\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(f\"Applicability Domain (OOD={ood_rate:.1%})\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Confidence vs correctness\n",
    "max_probs = test_probs.max(axis=1)\n",
    "correct = (test_preds == y_test)\n",
    "bins_edge = np.linspace(0, 1, 11)\n",
    "bin_accs, bin_confs = [], []\n",
    "for lo, hi in zip(bins_edge[:-1], bins_edge[1:]):\n",
    "    mask = (max_probs >= lo) & (max_probs < hi)\n",
    "    if mask.sum() > 0:\n",
    "        bin_accs.append(correct[mask].mean())\n",
    "        bin_confs.append(max_probs[mask].mean())\n",
    "axes[2].plot(bin_confs, bin_accs, \"o-\", color=\"#e67e22\", lw=2, label=\"Model\")\n",
    "axes[2].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5, label=\"Perfect\")\n",
    "axes[2].set_xlabel(\"Mean Confidence\")\n",
    "axes[2].set_ylabel(\"Accuracy\")\n",
    "axes[2].set_title(\"Reliability Diagram\")\n",
    "axes[2].legend()\n",
    "axes[2].set_xlim([-0.02, 1.02])\n",
    "axes[2].set_ylim([-0.02, 1.02])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"07_uncertainty.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Save model artifacts for reuse ────────────────────────────────────\nmodel_artifacts = {\n    \"best_model\": best_model,\n    \"calibrated_model\": calibrated,\n    \"selected_columns\": selected_columns,\n    \"activity_class_map\": ACTIVITY_CLASS_MAP,\n    \"num_classes\": NUM_CLASSES,\n    \"ad_threshold\": ad_threshold,\n    \"nn_model\": nn,\n    \"conformal_q\": q_threshold,\n    \"best_model_name\": best_model_name,\n}\nmodel_path = MODEL_DIR / \"safety_model.pkl\"\nwith open(model_path, \"wb\") as fh:\n    pickle.dump(model_artifacts, fh)\nprint(f\"Model saved to: {model_path}\")\nprint(f\"  Model type       : {best_model_name}\")\nprint(f\"  Feature columns  : {len(selected_columns)}\")\nprint(f\"  AD threshold     : {ad_threshold:.4f}\")\nprint(f\"  Conformal q      : {q_threshold:.4f}\")\n\n# ── Save summary JSON ─────────────────────────────────────────────────\nclass_counts_dict = {ACTIVITY_CLASS_MAP[c]: int((labels_all == c).sum()) for c in range(NUM_CLASSES)}\nsummary = {\n    \"n_compounds\": len(data),\n    \"targets\": sorted(set(targets_all)),\n    \"class_distribution\": class_counts_dict,\n    \"train_size\": len(X_train),\n    \"val_size\": len(X_val),\n    \"test_size\": len(X_test),\n    \"best_model\": best_model_name,\n    \"test_metrics\": {\n        \"roc_auc\": test_roc,\n        \"pr_auc\": test_pr,\n        \"mcc\": test_mcc,\n        \"ece\": ece,\n        \"mce\": mce,\n    },\n    \"conformal_coverage\": coverage,\n    \"avg_prediction_set_size\": float(set_sizes.mean()),\n    \"out_of_domain_rate\": ood_rate,\n}\nwith open(OUTPUT_DIR / \"workflow_summary.json\", \"w\") as fh:\n    json.dump(summary, fh, indent=2)\nprint(f\"\\nWorkflow summary saved to: {OUTPUT_DIR / 'workflow_summary.json'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 10. Test Set Evaluation (Held-Out Compounds)\n\nEvaluate the trained model on the held-out test set (`data/test_compounds.csv`).\nThese compounds were **not** used during training or cross-validation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Evaluate on held-out test set ─────────────────────────────────────\nif TEST_PATH.exists():\n    test_df = pd.read_csv(TEST_PATH)\n    print(f\"Loaded {len(test_df)} test compounds from {TEST_PATH}\")\n    print(f\"Test targets: {sorted(test_df['target'].unique())}\")\n\n    # Build feature matrix for test compounds\n    test_rows_for_features = []\n    for _, row in test_df.iterrows():\n        # Map old 3-class labels to 2-class: old class 2 -> 1 (binding), old class 0,1 -> 0 (non_binding)\n        raw_class = int(row.get(\"known_class\", -1)) if pd.notna(row.get(\"known_class\")) else -1\n        if raw_class >= 0:\n            mapped_class = 1 if raw_class == 2 else 0\n        else:\n            mapped_class = -1\n        test_rows_for_features.append({\n            \"canonical_smiles\": row[\"smiles\"],\n            \"target_common_name\": row[\"target\"],\n            \"activity_class\": mapped_class,\n        })\n    X_test_ext, y_test_ext, _ = build_feature_matrix(test_rows_for_features, selected_columns=selected_columns)\n\n    # Filter to only compounds with known labels\n    valid_mask = y_test_ext >= 0\n    X_test_ext_valid = X_test_ext[valid_mask]\n    y_test_ext_valid = y_test_ext[valid_mask]\n    test_df_valid = test_df[valid_mask].copy()\n\n    if len(y_test_ext_valid) > 0:\n        # Predict\n        ext_probs = best_model.predict_proba(X_test_ext_valid)\n        ext_preds = best_model.predict(X_test_ext_valid)\n\n        # Applicability domain\n        ext_dists = nn.kneighbors(X_test_ext_valid)[0].mean(axis=1)\n        ext_in_domain = ext_dists <= ad_threshold\n\n        # Metrics\n        ext_roc = roc_auc_score(y_test_ext_valid, ext_probs[:, 1])\n        ext_mcc = matthews_corrcoef(y_test_ext_valid, ext_preds)\n        ext_acc = (ext_preds == y_test_ext_valid).mean()\n\n        print(f\"\\nHeld-out Test Set Metrics ({best_model_name}):\")\n        print(f\"  Compounds evaluated: {len(y_test_ext_valid)}\")\n        print(f\"  ROC-AUC            : {ext_roc:.4f}\")\n        print(f\"  MCC                : {ext_mcc:.4f}\")\n        print(f\"  Accuracy           : {ext_acc:.4f}\")\n        print(f\"  In-domain          : {ext_in_domain.sum()}/{len(ext_in_domain)} ({ext_in_domain.mean():.1%})\")\n\n        # Per-target metrics\n        test_target_metrics = {}\n        for target in sorted(test_df_valid[\"target\"].unique()):\n            tmask = test_df_valid[\"target\"].values == target\n            if tmask.sum() < 2:\n                continue\n            t_true = y_test_ext_valid[tmask]\n            t_pred = ext_preds[tmask]\n            t_acc = (t_pred == t_true).mean()\n            t_mcc = matthews_corrcoef(t_true, t_pred) if len(set(t_true)) > 1 else 0.0\n            test_target_metrics[target] = {\"n\": int(tmask.sum()), \"acc\": t_acc, \"mcc\": t_mcc}\n\n        print(f\"\\nPer-target test set performance:\")\n        print(f\"  {'Target':>10s}  {'N':>4s}  {'Acc':>6s}  {'MCC':>6s}\")\n        print(f\"  {'-'*30}\")\n        for target, m in sorted(test_target_metrics.items()):\n            print(f\"  {target:>10s}  {m['n']:>4d}  {m['acc']:>.3f}  {m['mcc']:>.3f}\")\n\n        # Confusion matrix\n        print(f\"\\nClassification Report (held-out test set):\")\n        present_classes = sorted(set(y_test_ext_valid) | set(ext_preds))\n        target_names_present = [ACTIVITY_CLASS_MAP[c] for c in present_classes]\n        print(classification_report(\n            y_test_ext_valid, ext_preds,\n            labels=present_classes,\n            target_names=target_names_present,\n            digits=3, zero_division=0,\n        ))\n\n        # Build results DataFrame\n        test_df_valid = test_df_valid.copy()\n        test_df_valid[\"predicted_class\"] = ext_preds\n        test_df_valid[\"predicted_label\"] = [ACTIVITY_CLASS_MAP.get(int(p), \"?\") for p in ext_preds]\n        test_df_valid[\"correct\"] = ext_preds == y_test_ext_valid\n        test_df_valid[\"in_domain\"] = ext_in_domain\n        for c in range(NUM_CLASSES):\n            test_df_valid[f\"prob_{ACTIVITY_CLASS_MAP[c]}\"] = ext_probs[:, c]\n        test_df_valid[\"max_confidence\"] = ext_probs.max(axis=1)\n\n        # Save\n        test_df_valid.to_csv(OUTPUT_DIR / \"test_set_predictions.csv\", index=False)\n        print(f\"\\nTest set predictions saved to: {OUTPUT_DIR / 'test_set_predictions.csv'}\")\n    else:\n        print(\"No valid test compounds with known labels found.\")\n        ext_roc = ext_mcc = ext_acc = float(\"nan\")\n        test_target_metrics = {}\nelse:\n    print(f\"No test file found at {TEST_PATH}\")\n    ext_roc = ext_mcc = ext_acc = float(\"nan\")\n    test_target_metrics = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Figure 8: Test set results visualization ─────────────────────────\nif TEST_PATH.exists() and len(y_test_ext_valid) > 0:\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    # Class distribution: actual vs predicted\n    actual_counts = Counter(y_test_ext_valid)\n    pred_counts_ext = Counter(ext_preds)\n    x_labels = [ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)]\n    x = np.arange(NUM_CLASSES)\n    w = 0.35\n    axes[0].bar(x - w/2, [actual_counts.get(c, 0) for c in range(NUM_CLASSES)],\n                w, color=[CLASS_COLORS[c] for c in range(NUM_CLASSES)], edgecolor=\"black\",\n                alpha=0.7, label=\"Actual\")\n    axes[0].bar(x + w/2, [pred_counts_ext.get(c, 0) for c in range(NUM_CLASSES)],\n                w, color=[CLASS_COLORS[c] for c in range(NUM_CLASSES)], edgecolor=\"black\",\n                alpha=0.4, label=\"Predicted\", hatch=\"//\")\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(x_labels)\n    axes[0].set_title(\"Test Set: Actual vs Predicted\")\n    axes[0].set_ylabel(\"Count\")\n    axes[0].legend()\n\n    # Per-target accuracy\n    if test_target_metrics:\n        targets_sorted = sorted(test_target_metrics.keys(),\n                                key=lambda t: test_target_metrics[t][\"acc\"], reverse=True)\n        accs = [test_target_metrics[t][\"acc\"] for t in targets_sorted]\n        ns = [test_target_metrics[t][\"n\"] for t in targets_sorted]\n        colors = [\"#3498db\" if a >= 0.5 else \"#e74c3c\" for a in accs]\n        bars = axes[1].barh(range(len(targets_sorted)), accs, color=colors, edgecolor=\"black\")\n        axes[1].set_yticks(range(len(targets_sorted)))\n        axes[1].set_yticklabels([f\"{t} (n={n})\" for t, n in zip(targets_sorted, ns)])\n        axes[1].axvline(0.5, color=\"gray\", ls=\"--\", lw=1)\n        axes[1].set_xlabel(\"Accuracy\")\n        axes[1].set_title(\"Per-Target Test Accuracy\")\n        axes[1].set_xlim([0, 1.05])\n\n    # Confidence distribution\n    axes[2].hist(ext_probs.max(axis=1), bins=20, color=\"#9b59b6\", edgecolor=\"black\", alpha=0.8)\n    axes[2].axvline(0.5, color=\"red\", ls=\"--\", lw=1.5, label=\"50% threshold\")\n    axes[2].set_xlabel(\"Max Confidence\")\n    axes[2].set_ylabel(\"Count\")\n    axes[2].set_title(\"Test Set Confidence\")\n    axes[2].legend()\n\n    fig.suptitle(\"Held-Out Test Set Results\", fontsize=14, y=1.02)\n    fig.tight_layout()\n    fig.savefig(OUTPUT_DIR / \"08_test_set_results.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n    print(\"Saved: outputs/08_test_set_results.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 11. Predict New Compounds\n\nProvide a CSV file with these columns:\n\n| Column | Description |\n|--------|-------------|\n| `compound_id` | Your identifier for the compound |\n| `smiles` | SMILES string |\n| `target` | One of the 24 trained targets (e.g. `hERG`, `CYP3A4`, `ERa`, `P-gp`) |\n\nAn example file is provided at `data/example_predictions.csv`.\n\nThe output will include:\n- `prob_non_binding`, `prob_binding` -- predicted class probabilities\n- `predicted_class` / `predicted_label` -- most likely class\n- `conformal_set` -- set of plausible classes at 95% confidence\n- `in_domain` -- whether the compound falls within the model's applicability domain\n- `max_confidence` -- highest class probability (a rough quality indicator)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standalone prediction from saved model ────────────────────────────\n",
    "# Set YOUR_CSV below and run this cell.\n",
    "# No training cells need to be run first.\n",
    "\n",
    "YOUR_CSV = NOTEBOOK_DIR / \"data\" / \"example_predictions.csv\"  # <-- change this\n",
    "SAVED_MODEL = MODEL_DIR / \"safety_model.pkl\"\n",
    "\n",
    "if SAVED_MODEL.exists() and YOUR_CSV.exists():\n",
    "    standalone_results = predict_compounds(YOUR_CSV, SAVED_MODEL)\n",
    "    standalone_results.to_csv(OUTPUT_DIR / \"standalone_predictions.csv\", index=False)\n",
    "    print(f\"Predictions saved to: {OUTPUT_DIR / 'standalone_predictions.csv'}\")\n",
    "    display(standalone_results)\n",
    "elif not SAVED_MODEL.exists():\n",
    "    print(f\"No saved model found at {SAVED_MODEL}. Run training cells first.\")\n",
    "else:\n",
    "    print(f\"No input CSV found at {YOUR_CSV}. Update the YOUR_CSV variable above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 12. Analysis Report (Markdown Output)\n\nGenerate a comprehensive analysis report as a Markdown file saved to the outputs directory.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Generate analysis Markdown report ─────────────────────────────────\nimport datetime\n\nreport_lines = []\nreport_lines.append(\"# OFFTOXv3 Analysis Report\")\nreport_lines.append(f\"\\n**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\")\nreport_lines.append(f\"**Best Model:** {best_model_name}\")\nreport_lines.append(f\"**Targets:** {len(TARGET_PANEL)} safety pharmacology targets\")\nreport_lines.append(f\"**Classification:** Binary (binding vs non-binding at 10 uM threshold)\")\nreport_lines.append(\"\")\n\n# Dataset summary\nreport_lines.append(\"## 1. Dataset Summary\")\nreport_lines.append(\"\")\nreport_lines.append(f\"- **Total training compounds:** {len(data)}\")\nreport_lines.append(f\"- **Unique targets:** {len(set(targets_all))}\")\nreport_lines.append(f\"- **Train/Val/Test split:** {len(X_train)}/{len(X_val)}/{len(X_test)} (scaffold-based)\")\nreport_lines.append(f\"- **Feature dimensions:** {features.shape[1]} (10 descriptors + 2048 Morgan FP + target encoding)\")\nreport_lines.append(\"\")\n\n# Class distribution\nreport_lines.append(\"### Class Distribution\")\nreport_lines.append(\"\")\nreport_lines.append(\"| Class | Label | Count | Percentage |\")\nreport_lines.append(\"|-------|-------|------:|----------:|\")\nfor cls in range(NUM_CLASSES):\n    n = int((labels_all == cls).sum())\n    pct = 100 * n / len(data)\n    report_lines.append(f\"| {cls} | {ACTIVITY_CLASS_MAP[cls]} | {n} | {pct:.1f}% |\")\nreport_lines.append(\"\")\n\n# Per-target breakdown\nreport_lines.append(\"### Per-Target Compound Counts\")\nreport_lines.append(\"\")\nreport_lines.append(\"| Target | Category | Total | Binding | Non-Binding |\")\nreport_lines.append(\"|--------|----------|------:|--------:|------------:|\")\ntarget_df = pd.DataFrame({\"target\": targets_all, \"class\": labels_all})\nfor t in sorted(set(targets_all)):\n    cat = TARGET_PANEL.get(t, {}).get(\"category\", \"?\")\n    t_data = target_df[target_df[\"target\"] == t]\n    total = len(t_data)\n    binding = int((t_data[\"class\"] == 1).sum())\n    non_binding = int((t_data[\"class\"] == 0).sum())\n    report_lines.append(f\"| {t} | {cat} | {total} | {binding} | {non_binding} |\")\nreport_lines.append(\"\")\n\n# Cross-validation results\nreport_lines.append(\"## 2. Cross-Validation Results\")\nreport_lines.append(\"\")\nreport_lines.append(\"| Model | ROC-AUC | PR-AUC | MCC |\")\nreport_lines.append(\"|-------|--------:|-------:|----:|\")\nfor row in cv_summary_sorted:\n    report_lines.append(\n        f\"| {row['model']} | \"\n        f\"{row['roc_auc_mean']:.4f} +/- {row['roc_auc_std']:.4f} | \"\n        f\"{row['pr_auc_mean']:.4f} +/- {row['pr_auc_std']:.4f} | \"\n        f\"{row['mcc_mean']:.4f} +/- {row['mcc_std']:.4f} |\"\n    )\nreport_lines.append(\"\")\nreport_lines.append(f\"**Selected model:** {best_model_name} (highest ROC-AUC)\")\nreport_lines.append(\"\")\n\n# Internal test set metrics\nreport_lines.append(\"## 3. Internal Test Set Performance (Scaffold Split)\")\nreport_lines.append(\"\")\nreport_lines.append(f\"| Metric | Value |\")\nreport_lines.append(f\"|--------|------:|\")\nreport_lines.append(f\"| ROC-AUC | {test_roc:.4f} |\")\nreport_lines.append(f\"| PR-AUC | {test_pr:.4f} |\")\nreport_lines.append(f\"| MCC | {test_mcc:.4f} |\")\nreport_lines.append(f\"| ECE (calibrated) | {ece:.4f} |\")\nreport_lines.append(f\"| MCE (calibrated) | {mce:.4f} |\")\nreport_lines.append(\"\")\n\n# Confusion matrix\nreport_lines.append(\"### Confusion Matrix\")\nreport_lines.append(\"\")\ncm_report = confusion_matrix(y_test, test_preds, labels=list(range(NUM_CLASSES)))\nheader = \"| | \" + \" | \".join(f\"Pred: {ACTIVITY_CLASS_MAP[c]}\" for c in range(NUM_CLASSES)) + \" |\"\nreport_lines.append(header)\nreport_lines.append(\"|---|\" + \"---:|\" * NUM_CLASSES)\nfor i in range(NUM_CLASSES):\n    row_vals = \" | \".join(str(cm_report[i, j]) for j in range(NUM_CLASSES))\n    report_lines.append(f\"| **{ACTIVITY_CLASS_MAP[i]}** | {row_vals} |\")\nreport_lines.append(\"\")\n\n# Uncertainty\nreport_lines.append(\"## 4. Uncertainty Quantification\")\nreport_lines.append(\"\")\nreport_lines.append(f\"- **Conformal coverage:** {coverage:.4f} (target: 0.95)\")\nreport_lines.append(f\"- **Average prediction set size:** {set_sizes.mean():.2f}\")\nreport_lines.append(f\"- **AD threshold (95th pct k-NN):** {ad_threshold:.4f}\")\nreport_lines.append(f\"- **Out-of-domain rate:** {ood_rate:.2%}\")\nreport_lines.append(\"\")\n\n# Held-out test set\nreport_lines.append(\"## 5. Held-Out Test Set Evaluation\")\nreport_lines.append(\"\")\nif not np.isnan(ext_roc):\n    report_lines.append(f\"- **Test compounds:** {len(y_test_ext_valid)}\")\n    report_lines.append(f\"- **ROC-AUC:** {ext_roc:.4f}\")\n    report_lines.append(f\"- **MCC:** {ext_mcc:.4f}\")\n    report_lines.append(f\"- **Accuracy:** {ext_acc:.4f}\")\n    report_lines.append(\"\")\n    if test_target_metrics:\n        report_lines.append(\"### Per-Target Test Performance\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Target | N | Accuracy | MCC |\")\n        report_lines.append(\"|--------|--:|---------:|----:|\")\n        for target, m in sorted(test_target_metrics.items()):\n            report_lines.append(f\"| {target} | {m['n']} | {m['acc']:.3f} | {m['mcc']:.3f} |\")\n        report_lines.append(\"\")\nelse:\n    report_lines.append(\"No held-out test set was available for evaluation.\")\n    report_lines.append(\"\")\n\n# Statistical comparison\nreport_lines.append(\"## 6. Statistical Model Comparison\")\nreport_lines.append(\"\")\nif stat_rows:\n    report_lines.append(f\"Bonferroni-corrected alpha = {bonferroni:.4f}\")\n    report_lines.append(\"\")\n    report_lines.append(\"| Model A | Model B | t-stat | p-value | Cohen's d | Significant |\")\n    report_lines.append(\"|---------|---------|-------:|--------:|----------:|:-----------:|\")\n    for r in stat_rows:\n        report_lines.append(\n            f\"| {r['Model A']} | {r['Model B']} | \"\n            f\"{r['t-stat']:.4f} | {r['p-value']:.6f} | \"\n            f\"{r[\\\"Cohen's d\\\"]:.4f} | {r['Significant']} |\"\n        )\n    report_lines.append(\"\")\n\n# MCDA\nreport_lines.append(\"## 7. MCDA Ranking\")\nreport_lines.append(\"\")\nreport_lines.append(\"| Rank | Model | Composite Score |\")\nreport_lines.append(\"|-----:|-------|----------------:|\")\nfor i, r in enumerate(mcda_rows, 1):\n    report_lines.append(f\"| {i} | {r['model']} | {r['composite']:.4f} |\")\nreport_lines.append(\"\")\n\n# Target panel reference\nreport_lines.append(\"## 8. Target Panel Reference\")\nreport_lines.append(\"\")\nreport_lines.append(\"| # | Target | ChEMBL ID | Category |\")\nreport_lines.append(\"|--:|--------|-----------|----------|\")\nfor i, (tname, tinfo) in enumerate(TARGET_PANEL.items(), 1):\n    report_lines.append(f\"| {i} | {tname} | {tinfo['chembl_id']} | {tinfo['category']} |\")\nreport_lines.append(\"\")\n\n# Outputs\nreport_lines.append(\"## 9. Output Files\")\nreport_lines.append(\"\")\nreport_lines.append(\"| File | Description |\")\nreport_lines.append(\"|------|-------------|\")\nreport_lines.append(\"| `01_data_exploration.png` | Class distribution, per-target breakdown, pChEMBL histogram |\")\nreport_lines.append(\"| `02_roc_curves.png` | Per-class ROC curves |\")\nreport_lines.append(\"| `03_pr_curves.png` | Per-class Precision-Recall curves |\")\nreport_lines.append(\"| `04_confusion_matrix.png` | Confusion matrices (counts and percentages) |\")\nreport_lines.append(\"| `05_calibration_curves.png` | Per-class calibration curves |\")\nreport_lines.append(\"| `06_feature_importance.png` | Top 20 feature importances |\")\nreport_lines.append(\"| `07_uncertainty.png` | Conformal sets, AD distances, reliability diagram |\")\nreport_lines.append(\"| `08_test_set_results.png` | Held-out test set results |\")\nreport_lines.append(\"| `workflow_summary.json` | Machine-readable summary of all metrics |\")\nreport_lines.append(\"| `test_set_predictions.csv` | Held-out test set predictions with probabilities |\")\nreport_lines.append(\"| `analysis_report.md` | This report |\")\nreport_lines.append(\"\")\n\n# Write the report\nreport_path = OUTPUT_DIR / \"analysis_report.md\"\nwith open(report_path, \"w\") as f:\n    f.write(\"\\n\".join(report_lines))\n\nprint(f\"Analysis report saved to: {report_path}\")\nprint(f\"Report length: {len(report_lines)} lines\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}