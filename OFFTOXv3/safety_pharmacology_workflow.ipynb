{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OFFTOXv3 — Safety Pharmacology & NHR Prediction Workflow\n\nEnd-to-end workflow for predicting compound activity against **13 safety/toxicology targets** using a 3-class scheme:\n\n| Class | Label | Definition |\n|-------|-------|------------|\n| 2 | **potent** | pChEMBL >= 5.0 (IC50/Ki < 10 µM) |\n| 1 | **less_potent** | 4.0 <= pChEMBL < 5.0 (10–100 µM) |\n| 0 | **inactive** | Confirmed inactive (tested >= 10 µM, no activity) |\n\n**Targets covered:**\n- Cardiac ion channels: hERG, Cav1.2, Nav1.5\n- CYP enzymes: CYP3A4, CYP2D6, CYP1A2\n- Nuclear Hormone Receptors: ERa, AR, PR, PPARg, RXRa, PXR, GR\n\n---\n\n## How to use this notebook\n\n1. **Run cells 1–8** to train and evaluate models on the bundled training data.\n2. **Cell 9** (Predict New Compounds) accepts any CSV with `compound_id`, `smiles`, and `target` columns.\n3. All visualizations render inline. Outputs are also saved to `outputs/`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Install dependencies (run once) ──────────────────────────────────\n",
    "# Uncomment the line below if running for the first time:\n",
    "# !pip install numpy<2 pandas scikit-learn xgboost lightgbm rdkit-pypi matplotlib seaborn scipy joblib\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Crippen, Descriptors, Lipinski, MolSurf, rdFingerprintGenerator\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "%matplotlib inline\n",
    "\n",
    "# ── Paths ────────────────────────────────────────────────────────────\n",
    "NOTEBOOK_DIR = Path(\".\").resolve()\n",
    "DATA_PATH = NOTEBOOK_DIR / \"data\" / \"safety_targets_bioactivity.csv\"\n",
    "OUTPUT_DIR = NOTEBOOK_DIR / \"outputs\"\n",
    "MODEL_DIR  = NOTEBOOK_DIR / \"model\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── Constants ─────────────────────────────────────────────────────────\n",
    "RANDOM_STATE = 42\n",
    "ACTIVITY_CLASS_MAP = {0: \"inactive\", 1: \"less_potent\", 2: \"potent\"}\n",
    "CLASS_COLORS = {0: \"#2ecc71\", 1: \"#f39c12\", 2: \"#e74c3c\"}\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "print(f\"Data path  : {DATA_PATH}\")\n",
    "print(f\"Output dir : {OUTPUT_DIR}\")\n",
    "print(f\"Model dir  : {MODEL_DIR}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Core data structures ─────────────────────────────────────────────\n",
    "@dataclass\n",
    "class SplitData:\n",
    "    train_idx: np.ndarray\n",
    "    val_idx: np.ndarray\n",
    "    test_idx: np.ndarray\n",
    "\n",
    "\n",
    "def load_and_clean_data(path: Path) -> List[dict]:\n",
    "    \"\"\"Load training CSV and assign 3-class activity labels.\n",
    "\n",
    "    Classes:\n",
    "        2 - potent:      pChEMBL >= 5.0  (< 10 uM)\n",
    "        1 - less_potent: 4.0 <= pChEMBL < 5.0  (10-100 uM)\n",
    "        0 - inactive:    confirmed-inactive compounds\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with path.open(newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        for row in reader:\n",
    "            smi = row.get(\"canonical_smiles\")\n",
    "            if not smi:\n",
    "                continue\n",
    "\n",
    "            raw_class = row.get(\"activity_class\", \"\")\n",
    "            if raw_class == \"0\" or row.get(\"activity_class_label\") == \"inactive\":\n",
    "                row[\"pchembl_value\"] = None\n",
    "                row[\"activity_class\"] = 0\n",
    "                rows.append(row)\n",
    "                continue\n",
    "\n",
    "            if row.get(\"standard_relation\") != \"=\":\n",
    "                continue\n",
    "            if not row.get(\"pchembl_value\"):\n",
    "                continue\n",
    "            try:\n",
    "                pchembl = float(row[\"pchembl_value\"])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if pchembl < 4.0:\n",
    "                continue\n",
    "            row[\"pchembl_value\"] = pchembl\n",
    "            row[\"activity_class\"] = 2 if pchembl >= 5.0 else 1\n",
    "            rows.append(row)\n",
    "\n",
    "    deduped: dict = {}\n",
    "    for row in rows:\n",
    "        key = (row.get(\"molecule_chembl_id\"), row.get(\"target_chembl_id\"))\n",
    "        existing = deduped.get(key)\n",
    "        if existing is None:\n",
    "            deduped[key] = row\n",
    "        else:\n",
    "            existing_p = existing.get(\"pchembl_value\")\n",
    "            current_p = row.get(\"pchembl_value\")\n",
    "            if current_p is not None and (existing_p is None or current_p > existing_p):\n",
    "                deduped[key] = row\n",
    "    return list(deduped.values())\n",
    "\n",
    "\n",
    "# ── Load ──────────────────────────────────────────────────────────────\n",
    "data = load_and_clean_data(DATA_PATH)\n",
    "labels_all = np.array([row[\"activity_class\"] for row in data], dtype=int)\n",
    "targets_all = [row.get(\"target_common_name\", \"unknown\") for row in data]\n",
    "\n",
    "print(f\"Loaded {len(data)} compound-target records\")\n",
    "print(f\"Targets: {sorted(set(targets_all))}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for cls in sorted(ACTIVITY_CLASS_MAP):\n",
    "    n = int((labels_all == cls).sum())\n",
    "    print(f\"  {cls} ({ACTIVITY_CLASS_MAP[cls]:>12s}): {n:>5d}  ({100*n/len(data):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exploratory visualizations ────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Class distribution\n",
    "class_counts = Counter(labels_all)\n",
    "bars = axes[0].bar(\n",
    "    [ACTIVITY_CLASS_MAP[c] for c in sorted(class_counts)],\n",
    "    [class_counts[c] for c in sorted(class_counts)],\n",
    "    color=[CLASS_COLORS[c] for c in sorted(class_counts)],\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "for bar, c in zip(bars, sorted(class_counts)):\n",
    "    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 10,\n",
    "                 str(class_counts[c]), ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"Class Distribution\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# 2. Per-target breakdown\n",
    "target_class_df = pd.DataFrame({\"target\": targets_all, \"class\": labels_all})\n",
    "target_order = sorted(set(targets_all))\n",
    "class_by_target = target_class_df.groupby([\"target\", \"class\"]).size().unstack(fill_value=0)\n",
    "class_by_target = class_by_target.reindex(columns=[0, 1, 2], fill_value=0)\n",
    "class_by_target.columns = [ACTIVITY_CLASS_MAP[c] for c in class_by_target.columns]\n",
    "class_by_target.loc[target_order].plot.barh(\n",
    "    stacked=True, ax=axes[1],\n",
    "    color=[CLASS_COLORS[0], CLASS_COLORS[1], CLASS_COLORS[2]],\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[1].set_title(\"Compounds per Target\")\n",
    "axes[1].set_xlabel(\"Count\")\n",
    "axes[1].legend(title=\"Class\", loc=\"lower right\")\n",
    "\n",
    "# 3. pChEMBL distribution (active compounds only)\n",
    "pchembl_vals = [float(row[\"pchembl_value\"]) for row in data if row[\"pchembl_value\"] is not None]\n",
    "axes[2].hist(pchembl_vals, bins=30, color=\"#3498db\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[2].axvline(5.0, color=\"red\", ls=\"--\", lw=2, label=\"Potent threshold (5.0)\")\n",
    "axes[2].axvline(4.0, color=\"orange\", ls=\"--\", lw=2, label=\"Less-potent threshold (4.0)\")\n",
    "axes[2].set_title(\"pChEMBL Value Distribution\")\n",
    "axes[2].set_xlabel(\"pChEMBL\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"01_data_exploration.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: outputs/01_data_exploration.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors(smiles: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Compute 10 physicochemical descriptors per molecule.\"\"\"\n",
    "    descriptor_functions = {\n",
    "        \"MW\": Descriptors.MolWt,\n",
    "        \"LogP\": Crippen.MolLogP,\n",
    "        \"HBA\": Lipinski.NumHAcceptors,\n",
    "        \"HBD\": Lipinski.NumHDonors,\n",
    "        \"TPSA\": MolSurf.TPSA,\n",
    "        \"RotatableBonds\": Lipinski.NumRotatableBonds,\n",
    "        \"AromaticRings\": Lipinski.NumAromaticRings,\n",
    "        \"HeavyAtoms\": Lipinski.HeavyAtomCount,\n",
    "        \"FractionCSP3\": Lipinski.FractionCSP3,\n",
    "        \"MolMR\": Crippen.MolMR,\n",
    "    }\n",
    "    rows = []\n",
    "    for smi in smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            rows.append([np.nan] * len(descriptor_functions))\n",
    "            continue\n",
    "        rows.append([func(mol) for func in descriptor_functions.values()])\n",
    "    return np.array(rows, dtype=float), list(descriptor_functions.keys())\n",
    "\n",
    "\n",
    "def compute_morgan_fingerprints(smiles: List[str], n_bits: int = 2048) -> np.ndarray:\n",
    "    \"\"\"Compute 2048-bit Morgan fingerprints (ECFP4, radius=2).\"\"\"\n",
    "    gen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=n_bits)\n",
    "    fps = []\n",
    "    for smi in smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            fps.append(np.zeros(n_bits, dtype=int))\n",
    "            continue\n",
    "        fps.append(np.array(gen.GetFingerprint(mol)))\n",
    "    return np.array(fps)\n",
    "\n",
    "\n",
    "def build_feature_matrix(\n",
    "    rows: List[dict],\n",
    "    selected_columns: Optional[List[str]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Build combined feature matrix: descriptors + fingerprints + target encoding.\n",
    "\n",
    "    When selected_columns is None (training), variance filtering is applied and\n",
    "    the surviving column names are returned.  When selected_columns is provided\n",
    "    (prediction), the matrix is aligned to those columns.\n",
    "    \"\"\"\n",
    "    smiles = [row[\"canonical_smiles\"] for row in rows]\n",
    "    targets = [row.get(\"target_common_name\", row.get(\"target\", \"\")) for row in rows]\n",
    "    labels = np.array([row.get(\"activity_class\", -1) for row in rows], dtype=int)\n",
    "\n",
    "    descriptors, desc_names = compute_descriptors(smiles)\n",
    "    fingerprints = compute_morgan_fingerprints(smiles)\n",
    "    fp_names = [f\"FP_{i}\" for i in range(fingerprints.shape[1])]\n",
    "\n",
    "    target_names = sorted({t for t in targets if t})\n",
    "    target_map = {name: idx for idx, name in enumerate(target_names)}\n",
    "    target_matrix = np.zeros((len(rows), len(target_names)), dtype=float)\n",
    "    for idx, target in enumerate(targets):\n",
    "        if target in target_map:\n",
    "            target_matrix[idx, target_map[target]] = 1.0\n",
    "\n",
    "    feature_matrix = np.concatenate([descriptors, fingerprints, target_matrix], axis=1)\n",
    "    columns = desc_names + fp_names + [f\"target_{n}\" for n in target_names]\n",
    "\n",
    "    if selected_columns is None:\n",
    "        variances = np.nanvar(feature_matrix, axis=0)\n",
    "        mask = variances > 0.01\n",
    "        feature_matrix = np.nan_to_num(feature_matrix[:, mask], nan=0.0)\n",
    "        selected_columns = [col for col, keep in zip(columns, mask) if keep]\n",
    "    else:\n",
    "        col_index = {col: idx for idx, col in enumerate(columns)}\n",
    "        aligned = np.zeros((len(rows), len(selected_columns)), dtype=float)\n",
    "        for out_idx, col in enumerate(selected_columns):\n",
    "            if col in col_index:\n",
    "                aligned[:, out_idx] = np.nan_to_num(\n",
    "                    feature_matrix[:, col_index[col]], nan=0.0\n",
    "                )\n",
    "        feature_matrix = aligned\n",
    "\n",
    "    return feature_matrix, labels, selected_columns\n",
    "\n",
    "\n",
    "# ── Build features ────────────────────────────────────────────────────\n",
    "print(\"Computing features (descriptors + 2048-bit Morgan FP + target encoding)...\")\n",
    "t0 = time.time()\n",
    "features, labels, selected_columns = build_feature_matrix(data)\n",
    "print(f\"  Done in {time.time() - t0:.1f}s\")\n",
    "print(f\"  Feature matrix shape: {features.shape}\")\n",
    "print(f\"  Features retained after variance filter: {len(selected_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Scaffold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split(smiles: List[str], y: np.ndarray, random_state: int = 42) -> SplitData:\n",
    "    \"\"\"Split data by Murcko scaffold to avoid data leakage (60/20/20).\"\"\"\n",
    "    scaffolds: Dict[str, List[int]] = {}\n",
    "    for idx, smi in enumerate(smiles):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            scaffold = \"\"\n",
    "        else:\n",
    "            scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol)\n",
    "        scaffolds.setdefault(scaffold, []).append(idx)\n",
    "\n",
    "    scaffold_sets = sorted(scaffolds.values(), key=len, reverse=True)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    rng.shuffle(scaffold_sets)\n",
    "\n",
    "    n_total = len(smiles)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for group in scaffold_sets:\n",
    "        if len(train_idx) + len(group) <= n_train:\n",
    "            train_idx.extend(group)\n",
    "        elif len(val_idx) + len(group) <= n_val:\n",
    "            val_idx.extend(group)\n",
    "        else:\n",
    "            test_idx.extend(group)\n",
    "\n",
    "    return SplitData(\n",
    "        train_idx=np.array(train_idx),\n",
    "        val_idx=np.array(val_idx),\n",
    "        test_idx=np.array(test_idx),\n",
    "    )\n",
    "\n",
    "\n",
    "split = scaffold_split([row[\"canonical_smiles\"] for row in data], labels, RANDOM_STATE)\n",
    "\n",
    "X_train, y_train = features[split.train_idx], labels[split.train_idx]\n",
    "X_val, y_val     = features[split.val_idx],   labels[split.val_idx]\n",
    "X_test, y_test   = features[split.test_idx],  labels[split.test_idx]\n",
    "\n",
    "print(f\"Train : {len(X_train):>5d}  |  Val : {len(X_val):>5d}  |  Test : {len(X_test):>5d}\")\n",
    "for name, y_sub in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    counts = Counter(y_sub)\n",
    "    parts = \", \".join(f\"{ACTIVITY_CLASS_MAP[c]}={counts.get(c, 0)}\" for c in range(NUM_CLASSES))\n",
    "    print(f\"  {name:>5s}: {parts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Training & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ece_score_fn(y_true, y_prob, n_bins=10):\n    \"\"\"Expected Calibration Error and Maximum Calibration Error.\"\"\"\n    bins = np.linspace(0, 1, n_bins + 1)\n    binids = np.digitize(y_prob, bins) - 1\n    ece, mce = 0.0, 0.0\n    for i in range(n_bins):\n        mask = binids == i\n        if not np.any(mask):\n            continue\n        avg_conf = y_prob[mask].mean()\n        avg_acc = y_true[mask].mean()\n        gap = abs(avg_conf - avg_acc)\n        ece += gap * mask.mean()\n        mce = max(mce, gap)\n    return ece, mce\n\n\ndef get_models(random_state: int) -> Dict[str, Tuple[Pipeline, Dict[str, list]]]:\n    \"\"\"Return three model pipelines with hyperparameter search spaces.\"\"\"\n    return {\n        \"RandomForest\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", RandomForestClassifier(random_state=random_state, n_jobs=-1)),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [10, 20, None],\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__max_features\": [\"sqrt\", \"log2\", 0.3],\n                \"model__class_weight\": [\"balanced\", None],\n            },\n        ),\n        \"XGBoost\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", XGBClassifier(\n                    random_state=random_state,\n                    objective=\"multi:softprob\",\n                    num_class=NUM_CLASSES,\n                    eval_metric=\"mlogloss\",\n                    n_jobs=-1,\n                    verbosity=0,\n                )),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [3, 5, 7],\n                \"model__learning_rate\": [0.01, 0.05, 0.1],\n                \"model__subsample\": [0.6, 0.8, 1.0],\n                \"model__colsample_bytree\": [0.6, 0.8, 1.0],\n            },\n        ),\n        \"LightGBM\": (\n            Pipeline([\n                (\"scaler\", StandardScaler(with_mean=False)),\n                (\"model\", LGBMClassifier(\n                    random_state=random_state, n_jobs=-1, verbose=-1,\n                    objective=\"multiclass\",\n                    num_class=NUM_CLASSES,\n                )),\n            ]),\n            {\n                \"model__n_estimators\": [200, 500],\n                \"model__max_depth\": [-1, 5, 10],\n                \"model__learning_rate\": [0.01, 0.05, 0.1],\n                \"model__num_leaves\": [31, 63, 127],\n                \"model__subsample\": [0.6, 0.8, 1.0],\n            },\n        ),\n    }\n\n\n# ── Train all models ──────────────────────────────────────────────────\nmodels = get_models(RANDOM_STATE)\ncv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=RANDOM_STATE)\n\ncv_summary = []\nbest_estimators = {}\ncalibration_metrics = {}\nfold_scores: Dict[str, List[float]] = {}\ntrain_times = {}\n\nfor name, (pipeline, param_grid) in models.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {name}\")\n    print(f\"{'='*60}\")\n\n    # Hyperparameter search\n    search = RandomizedSearchCV(\n        pipeline,\n        param_distributions=param_grid,\n        n_iter=5,\n        scoring=\"roc_auc_ovr\",\n        cv=3,\n        random_state=RANDOM_STATE,\n        n_jobs=1,  # avoid over-subscription with model-level n_jobs=-1\n    )\n    t0 = time.time()\n    search.fit(X_train, y_train)\n    train_times[name] = time.time() - t0\n    best_estimators[name] = search.best_estimator_\n    print(f\"  Best params: {search.best_params_}\")\n    print(f\"  Train time : {train_times[name]:.1f}s\")\n\n    # Cross-validation evaluation\n    scores, pr_scores, mcc_scores = [], [], []\n    for train_idx, test_idx in cv.split(X_train, y_train):\n        X_tr, X_te = X_train[train_idx], X_train[test_idx]\n        y_tr, y_te = y_train[train_idx], y_train[test_idx]\n        est = search.best_estimator_\n        est.fit(X_tr, y_tr)\n        probs = est.predict_proba(X_te)\n        preds = est.predict(X_te)\n        scores.append(roc_auc_score(y_te, probs, multi_class=\"ovr\", average=\"macro\"))\n        pr_per = [average_precision_score((y_te == c).astype(int), probs[:, c])\n                  for c in range(NUM_CLASSES) if (y_te == c).sum() > 0]\n        pr_scores.append(float(np.mean(pr_per)) if pr_per else 0.0)\n        mcc_scores.append(matthews_corrcoef(y_te, preds))\n\n    cv_summary.append({\n        \"model\": name,\n        \"roc_auc_mean\": np.mean(scores), \"roc_auc_std\": np.std(scores),\n        \"pr_auc_mean\": np.mean(pr_scores), \"pr_auc_std\": np.std(pr_scores),\n        \"mcc_mean\": np.mean(mcc_scores), \"mcc_std\": np.std(mcc_scores),\n    })\n    fold_scores[name] = scores\n\n    # Validation calibration\n    val_probs = search.best_estimator_.predict_proba(X_val)\n    val_probs_true = val_probs[np.arange(len(y_val)), y_val]\n    ece_val, _ = ece_score_fn(np.ones(len(y_val)), val_probs_true)\n    calibration_metrics[name] = ece_val\n\n    print(f\"  CV ROC-AUC : {np.mean(scores):.4f} +/- {np.std(scores):.4f}\")\n    print(f\"  CV PR-AUC  : {np.mean(pr_scores):.4f} +/- {np.std(pr_scores):.4f}\")\n    print(f\"  CV MCC     : {np.mean(mcc_scores):.4f} +/- {np.std(mcc_scores):.4f}\")\n\nprint(f\"\\nTraining complete. {len(models)} models evaluated.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Evaluation & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Select best model & refit on train+val ────────────────────────────\n",
    "cv_summary_sorted = sorted(cv_summary, key=lambda r: r[\"roc_auc_mean\"], reverse=True)\n",
    "best_model_name = cv_summary_sorted[0][\"model\"]\n",
    "best_model = best_estimators[best_model_name]\n",
    "best_model.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n",
    "\n",
    "print(\"Cross-validation summary (sorted by ROC-AUC):\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Model':<15s} {'ROC-AUC':>18s} {'PR-AUC':>18s} {'MCC':>18s}\")\n",
    "print(\"-\" * 75)\n",
    "for row in cv_summary_sorted:\n",
    "    print(f\"{row['model']:<15s} \"\n",
    "          f\"{row['roc_auc_mean']:.4f} +/- {row['roc_auc_std']:.4f}  \"\n",
    "          f\"{row['pr_auc_mean']:.4f} +/- {row['pr_auc_std']:.4f}  \"\n",
    "          f\"{row['mcc_mean']:.4f} +/- {row['mcc_std']:.4f}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# ── Test-set evaluation ───────────────────────────────────────────────\n",
    "test_probs = best_model.predict_proba(X_test)\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_roc = roc_auc_score(y_test, test_probs, multi_class=\"ovr\", average=\"macro\")\n",
    "pr_per = [average_precision_score((y_test == c).astype(int), test_probs[:, c])\n",
    "          for c in range(NUM_CLASSES) if (y_test == c).sum() > 0]\n",
    "test_pr = float(np.mean(pr_per)) if pr_per else 0.0\n",
    "test_mcc = matthews_corrcoef(y_test, test_preds)\n",
    "\n",
    "# Calibration\n",
    "calibrated = CalibratedClassifierCV(best_model, method=\"isotonic\", cv=3)\n",
    "calibrated.fit(np.vstack([X_train, X_val]), np.hstack([y_train, y_val]))\n",
    "cal_probs = calibrated.predict_proba(X_test)\n",
    "cal_probs_true = cal_probs[np.arange(len(y_test)), y_test]\n",
    "ece, mce = ece_score_fn(np.ones(len(y_test)), cal_probs_true)\n",
    "\n",
    "print(f\"\\nTest Set Metrics ({best_model_name}):\")\n",
    "print(f\"  ROC-AUC (macro) : {test_roc:.4f}\")\n",
    "print(f\"  PR-AUC  (macro) : {test_pr:.4f}\")\n",
    "print(f\"  MCC             : {test_mcc:.4f}\")\n",
    "print(f\"  ECE (calibrated): {ece:.4f}\")\n",
    "print(f\"  MCE (calibrated): {mce:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, test_preds,\n",
    "    target_names=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n",
    "    digits=3,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 2: ROC Curves (per-class) ──────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for cls in range(NUM_CLASSES):\n",
    "    label = ACTIVITY_CLASS_MAP[cls]\n",
    "    binary_true = (y_test == cls).astype(int)\n",
    "    if binary_true.sum() == 0:\n",
    "        axes[cls].set_title(f\"ROC — {label} (no samples)\")\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(binary_true, test_probs[:, cls])\n",
    "    auc_val = roc_auc_score(binary_true, test_probs[:, cls])\n",
    "    axes[cls].plot(fpr, tpr, color=CLASS_COLORS[cls], lw=2,\n",
    "                   label=f\"AUC = {auc_val:.3f}\")\n",
    "    axes[cls].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5)\n",
    "    axes[cls].set_xlabel(\"False Positive Rate\")\n",
    "    axes[cls].set_ylabel(\"True Positive Rate\")\n",
    "    axes[cls].set_title(f\"ROC — {label}\")\n",
    "    axes[cls].legend(loc=\"lower right\", fontsize=12)\n",
    "    axes[cls].set_xlim([-0.02, 1.02])\n",
    "    axes[cls].set_ylim([-0.02, 1.02])\n",
    "\n",
    "fig.suptitle(f\"Per-Class ROC Curves ({best_model_name})\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"02_roc_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 3: Precision-Recall Curves ────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for cls in range(NUM_CLASSES):\n",
    "    label = ACTIVITY_CLASS_MAP[cls]\n",
    "    binary_true = (y_test == cls).astype(int)\n",
    "    if binary_true.sum() == 0:\n",
    "        axes[cls].set_title(f\"PR — {label} (no samples)\")\n",
    "        continue\n",
    "    precision, recall, _ = precision_recall_curve(binary_true, test_probs[:, cls])\n",
    "    ap = average_precision_score(binary_true, test_probs[:, cls])\n",
    "    axes[cls].plot(recall, precision, color=CLASS_COLORS[cls], lw=2,\n",
    "                   label=f\"AP = {ap:.3f}\")\n",
    "    baseline = binary_true.mean()\n",
    "    axes[cls].axhline(baseline, color=\"gray\", ls=\"--\", lw=1, alpha=0.5,\n",
    "                      label=f\"Baseline = {baseline:.3f}\")\n",
    "    axes[cls].set_xlabel(\"Recall\")\n",
    "    axes[cls].set_ylabel(\"Precision\")\n",
    "    axes[cls].set_title(f\"PR — {label}\")\n",
    "    axes[cls].legend(loc=\"upper right\", fontsize=11)\n",
    "    axes[cls].set_xlim([-0.02, 1.02])\n",
    "    axes[cls].set_ylim([-0.02, 1.02])\n",
    "\n",
    "fig.suptitle(f\"Per-Class Precision-Recall Curves ({best_model_name})\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"03_pr_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 4: Confusion Matrix ───────────────────────────────────────\n",
    "cm = confusion_matrix(y_test, test_preds, labels=list(range(NUM_CLASSES)))\n",
    "cm_pct = cm.astype(float) / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0],\n",
    "            xticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n",
    "            yticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)])\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "axes[0].set_title(\"Confusion Matrix (counts)\")\n",
    "\n",
    "# Percentages\n",
    "sns.heatmap(cm_pct, annot=True, fmt=\".1f\", cmap=\"Blues\", ax=axes[1],\n",
    "            xticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)],\n",
    "            yticklabels=[ACTIVITY_CLASS_MAP[c] for c in range(NUM_CLASSES)])\n",
    "axes[1].set_xlabel(\"Predicted\")\n",
    "axes[1].set_ylabel(\"Actual\")\n",
    "axes[1].set_title(\"Confusion Matrix (% per row)\")\n",
    "\n",
    "fig.suptitle(f\"Test Set Confusion Matrix ({best_model_name})\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"04_confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 5: Calibration Curves ─────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for cls in range(NUM_CLASSES):\n",
    "    label = ACTIVITY_CLASS_MAP[cls]\n",
    "    binary_true = (y_test == cls).astype(int)\n",
    "    cls_cal_probs = cal_probs[:, cls]\n",
    "    if binary_true.sum() == 0:\n",
    "        axes[cls].set_title(f\"Calibration — {label} (no samples)\")\n",
    "        continue\n",
    "    prob_true, prob_pred = calibration_curve(binary_true, cls_cal_probs, n_bins=10)\n",
    "    axes[cls].plot(prob_pred, prob_true, \"o-\", color=CLASS_COLORS[cls], lw=2,\n",
    "                   label=f\"{label}\")\n",
    "    axes[cls].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5, label=\"Perfectly calibrated\")\n",
    "    axes[cls].set_xlabel(\"Mean Predicted Probability\")\n",
    "    axes[cls].set_ylabel(\"Fraction of Positives\")\n",
    "    axes[cls].set_title(f\"Calibration — {label}\")\n",
    "    axes[cls].legend(loc=\"upper left\", fontsize=11)\n",
    "    axes[cls].set_xlim([-0.02, 1.02])\n",
    "    axes[cls].set_ylim([-0.02, 1.02])\n",
    "\n",
    "fig.suptitle(f\"Calibration Curves (isotonic, {best_model_name})\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"05_calibration_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"ECE = {ece:.4f}, MCE = {mce:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 6: Feature Importance (top 20) ────────────────────────────\n",
    "if hasattr(best_model.named_steps[\"model\"], \"feature_importances_\"):\n",
    "    importances = best_model.named_steps[\"model\"].feature_importances_\n",
    "    top_k = 20\n",
    "    indices = np.argsort(importances)[-top_k:]\n",
    "    top_features = [selected_columns[i] for i in indices]\n",
    "    top_values = importances[indices]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.barh(range(top_k), top_values, color=\"#3498db\", edgecolor=\"black\")\n",
    "    ax.set_yticks(range(top_k))\n",
    "    ax.set_yticklabels(top_features)\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {top_k} Feature Importances ({best_model_name})\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUTPUT_DIR / \"06_feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Statistical Comparison & MCDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Paired t-tests with Bonferroni correction ────────────────────────\n",
    "model_names = [row[\"model\"] for row in cv_summary_sorted]\n",
    "stat_rows = []\n",
    "for i, model_a in enumerate(model_names):\n",
    "    for model_b in model_names[i + 1:]:\n",
    "        sa = np.array(fold_scores.get(model_a, []))\n",
    "        sb = np.array(fold_scores.get(model_b, []))\n",
    "        if len(sa) == 0 or len(sb) == 0:\n",
    "            continue\n",
    "        t_stat, p_val = stats.ttest_rel(sa, sb)\n",
    "        pooled = np.std(np.concatenate([sa, sb]))\n",
    "        cohen_d = (sa.mean() - sb.mean()) / pooled if pooled else 0.0\n",
    "        stat_rows.append({\n",
    "            \"Model A\": model_a, \"Model B\": model_b,\n",
    "            \"t-stat\": t_stat, \"p-value\": p_val, \"Cohen's d\": cohen_d,\n",
    "        })\n",
    "\n",
    "if stat_rows:\n",
    "    bonferroni = 0.05 / len(stat_rows)\n",
    "    for r in stat_rows:\n",
    "        r[\"Significant\"] = \"Yes\" if r[\"p-value\"] < bonferroni else \"No\"\n",
    "        r[\"Bonferroni alpha\"] = bonferroni\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows)\n",
    "print(\"Statistical Comparison (paired t-test on CV ROC-AUC folds):\")\n",
    "print(f\"Bonferroni-corrected alpha = {bonferroni:.4f}\")\n",
    "display(stat_df.style.format({\n",
    "    \"t-stat\": \"{:.4f}\", \"p-value\": \"{:.6f}\",\n",
    "    \"Cohen's d\": \"{:.4f}\", \"Bonferroni alpha\": \"{:.4f}\",\n",
    "}).set_caption(\"Pairwise Model Comparison\"))\n",
    "\n",
    "# ── MCDA Ranking ─────────────────────────────────────────────────────\n",
    "mcda_rows = []\n",
    "for row in cv_summary_sorted:\n",
    "    name = row[\"model\"]\n",
    "    mcda_rows.append({\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": row[\"roc_auc_mean\"],\n",
    "        \"pr_auc\": row[\"pr_auc_mean\"],\n",
    "        \"calibration\": max(0.0, 1 - calibration_metrics.get(name, ece)),\n",
    "        \"robustness\": max(0.0, 1 - row[\"roc_auc_std\"]),\n",
    "        \"efficiency\": 1.0 / (1.0 + train_times.get(name, 1.0)),\n",
    "        \"interpretability\": 1.0 if name in {\"RandomForest\", \"LightGBM\", \"XGBoost\"} else 0.5,\n",
    "    })\n",
    "\n",
    "weights = {\n",
    "    \"roc_auc\": 0.25, \"pr_auc\": 0.20, \"calibration\": 0.20,\n",
    "    \"robustness\": 0.15, \"efficiency\": 0.10, \"interpretability\": 0.10,\n",
    "}\n",
    "for metric in weights:\n",
    "    vals = [r[metric] for r in mcda_rows]\n",
    "    mn, mx = min(vals), max(vals)\n",
    "    for r in mcda_rows:\n",
    "        r[metric] = (r[metric] - mn) / (mx - mn) if mx > mn else 1.0\n",
    "for r in mcda_rows:\n",
    "    r[\"composite\"] = sum(r[m] * w for m, w in weights.items())\n",
    "mcda_rows = sorted(mcda_rows, key=lambda r: r[\"composite\"], reverse=True)\n",
    "\n",
    "print(\"\\nMCDA Ranking:\")\n",
    "mcda_df = pd.DataFrame(mcda_rows)\n",
    "display(mcda_df.style.format(\"{:.4f}\", subset=mcda_df.columns[1:]).set_caption(\n",
    "    \"Multi-Criteria Decision Analysis\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Conformal Prediction ──────────────────────────────────────────────\n",
    "def conformal_prediction(probs, y_true, alpha=0.05):\n",
    "    scores = 1.0 - probs[np.arange(len(y_true)), y_true]\n",
    "    q = np.quantile(scores, 1 - alpha, method=\"higher\")\n",
    "    prediction_sets = probs >= (1.0 - q)\n",
    "    coverage = prediction_sets[np.arange(len(y_true)), y_true].mean()\n",
    "    return prediction_sets, coverage, q\n",
    "\n",
    "pred_sets, coverage, q_threshold = conformal_prediction(cal_probs, y_test)\n",
    "set_sizes = pred_sets.sum(axis=1)\n",
    "\n",
    "# ── Applicability Domain (k-NN distance) ─────────────────────────────\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_train)\n",
    "train_dists = nn.kneighbors(X_train)[0].mean(axis=1)\n",
    "ad_threshold = np.percentile(train_dists, 95)\n",
    "test_dists = nn.kneighbors(X_test)[0].mean(axis=1)\n",
    "ood_rate = (test_dists > ad_threshold).mean()\n",
    "\n",
    "print(f\"Conformal Prediction (alpha=0.05):\")\n",
    "print(f\"  Coverage          : {coverage:.4f}  (target: 0.95)\")\n",
    "print(f\"  Avg set size      : {set_sizes.mean():.2f}\")\n",
    "print(f\"  Quantile threshold: {q_threshold:.4f}\")\n",
    "print(f\"\\nApplicability Domain:\")\n",
    "print(f\"  AD threshold (95th pct): {ad_threshold:.4f}\")\n",
    "print(f\"  Out-of-domain rate     : {ood_rate:.2%}\")\n",
    "\n",
    "# ── Figure 7: Uncertainty plots ──────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Set size distribution\n",
    "unique_sizes, counts = np.unique(set_sizes, return_counts=True)\n",
    "axes[0].bar(unique_sizes.astype(str), counts, color=\"#9b59b6\", edgecolor=\"black\")\n",
    "for s, c in zip(unique_sizes, counts):\n",
    "    axes[0].text(str(s), c + 2, str(c), ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Prediction Set Size\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(f\"Conformal Set Sizes (coverage={coverage:.2%})\")\n",
    "\n",
    "# Distance to training set\n",
    "axes[1].hist(test_dists, bins=30, color=\"#1abc9c\", edgecolor=\"black\", alpha=0.8,\n",
    "             label=\"Test compounds\")\n",
    "axes[1].axvline(ad_threshold, color=\"red\", ls=\"--\", lw=2,\n",
    "                label=f\"AD threshold ({ad_threshold:.2f})\")\n",
    "axes[1].set_xlabel(\"Mean k-NN Distance\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(f\"Applicability Domain (OOD={ood_rate:.1%})\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Confidence vs correctness\n",
    "max_probs = test_probs.max(axis=1)\n",
    "correct = (test_preds == y_test)\n",
    "bins_edge = np.linspace(0, 1, 11)\n",
    "bin_accs, bin_confs = [], []\n",
    "for lo, hi in zip(bins_edge[:-1], bins_edge[1:]):\n",
    "    mask = (max_probs >= lo) & (max_probs < hi)\n",
    "    if mask.sum() > 0:\n",
    "        bin_accs.append(correct[mask].mean())\n",
    "        bin_confs.append(max_probs[mask].mean())\n",
    "axes[2].plot(bin_confs, bin_accs, \"o-\", color=\"#e67e22\", lw=2, label=\"Model\")\n",
    "axes[2].plot([0, 1], [0, 1], \"k--\", lw=1, alpha=0.5, label=\"Perfect\")\n",
    "axes[2].set_xlabel(\"Mean Confidence\")\n",
    "axes[2].set_ylabel(\"Accuracy\")\n",
    "axes[2].set_title(\"Reliability Diagram\")\n",
    "axes[2].legend()\n",
    "axes[2].set_xlim([-0.02, 1.02])\n",
    "axes[2].set_ylim([-0.02, 1.02])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / \"07_uncertainty.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save model artifacts for reuse ────────────────────────────────────\n",
    "model_artifacts = {\n",
    "    \"best_model\": best_model,\n",
    "    \"calibrated_model\": calibrated,\n",
    "    \"selected_columns\": selected_columns,\n",
    "    \"activity_class_map\": ACTIVITY_CLASS_MAP,\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"ad_threshold\": ad_threshold,\n",
    "    \"nn_model\": nn,\n",
    "    \"conformal_q\": q_threshold,\n",
    "    \"best_model_name\": best_model_name,\n",
    "}\n",
    "model_path = MODEL_DIR / \"safety_model.pkl\"\n",
    "with open(model_path, \"wb\") as fh:\n",
    "    pickle.dump(model_artifacts, fh)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"  Model type       : {best_model_name}\")\n",
    "print(f\"  Feature columns  : {len(selected_columns)}\")\n",
    "print(f\"  AD threshold     : {ad_threshold:.4f}\")\n",
    "print(f\"  Conformal q      : {q_threshold:.4f}\")\n",
    "\n",
    "# ── Save summary JSON ─────────────────────────────────────────────────\n",
    "class_counts_dict = {ACTIVITY_CLASS_MAP[c]: int((labels == c).sum()) for c in range(NUM_CLASSES)}\n",
    "summary = {\n",
    "    \"n_compounds\": len(data),\n",
    "    \"targets\": sorted(set(targets_all)),\n",
    "    \"class_distribution\": class_counts_dict,\n",
    "    \"train_size\": len(X_train),\n",
    "    \"val_size\": len(X_val),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"best_model\": best_model_name,\n",
    "    \"test_metrics\": {\n",
    "        \"roc_auc_macro\": test_roc,\n",
    "        \"pr_auc_macro\": test_pr,\n",
    "        \"mcc\": test_mcc,\n",
    "        \"ece\": ece,\n",
    "        \"mce\": mce,\n",
    "    },\n",
    "    \"conformal_coverage\": coverage,\n",
    "    \"avg_prediction_set_size\": float(set_sizes.mean()),\n",
    "    \"out_of_domain_rate\": ood_rate,\n",
    "}\n",
    "with open(OUTPUT_DIR / \"workflow_summary.json\", \"w\") as fh:\n",
    "    json.dump(summary, fh, indent=2)\n",
    "print(f\"\\nWorkflow summary saved to: {OUTPUT_DIR / 'workflow_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Predict New Compounds\n",
    "\n",
    "Provide a CSV file with these columns:\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `compound_id` | Your identifier for the compound |\n",
    "| `smiles` | SMILES string |\n",
    "| `target` | One of the trained targets (e.g. `hERG`, `CYP3A4`) |\n",
    "\n",
    "An example file is provided at `data/example_predictions.csv`.\n",
    "\n",
    "The output will include:\n",
    "- `prob_inactive`, `prob_less_potent`, `prob_potent` — predicted class probabilities\n",
    "- `predicted_class` / `predicted_label` — most likely class\n",
    "- `conformal_set` — set of plausible classes at 95% confidence\n",
    "- `in_domain` — whether the compound falls within the model's applicability domain\n",
    "- `max_confidence` — highest class probability (a rough quality indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── SET YOUR INPUT FILE HERE ─────────────────────────────────────────\nPREDICTION_CSV = NOTEBOOK_DIR / \"data\" / \"validation_compounds.csv\"\n# ─────────────────────────────────────────────────────────────────────\n\ndef predict_compounds(csv_path: Path, model_path: Path) -> pd.DataFrame:\n    \"\"\"Load a trained model and predict on new compounds from a CSV file.\n\n    Parameters\n    ----------\n    csv_path : Path\n        CSV with columns: compound_id, smiles, target\n    model_path : Path\n        Path to the saved safety_model.pkl\n\n    Returns\n    -------\n    pd.DataFrame\n        Predictions with probabilities, conformal sets, and AD flags.\n    \"\"\"\n    # Load model artifacts\n    with open(model_path, \"rb\") as fh:\n        arts = pickle.load(fh)\n\n    model = arts[\"best_model\"]\n    sel_cols = arts[\"selected_columns\"]\n    class_map = arts[\"activity_class_map\"]\n    n_cls = arts[\"num_classes\"]\n    ad_thresh = arts[\"ad_threshold\"]\n    nn_model = arts[\"nn_model\"]\n    conf_q = arts[\"conformal_q\"]\n\n    # Read user CSV\n    user_df = pd.read_csv(csv_path)\n    required = {\"compound_id\", \"smiles\", \"target\"}\n    missing = required - set(user_df.columns)\n    if missing:\n        raise ValueError(f\"CSV is missing required columns: {missing}\")\n\n    # Validate SMILES\n    valid_mask = []\n    for smi in user_df[\"smiles\"]:\n        mol = Chem.MolFromSmiles(smi)\n        valid_mask.append(mol is not None)\n    user_df[\"valid_smiles\"] = valid_mask\n    n_invalid = (~user_df[\"valid_smiles\"]).sum()\n    if n_invalid > 0:\n        print(f\"WARNING: {n_invalid} compounds have invalid SMILES and will get NaN predictions.\")\n\n    # Preserve extra columns for downstream use\n    extra_cols = [c for c in user_df.columns\n                  if c not in {\"compound_id\", \"smiles\", \"target\", \"valid_smiles\"}]\n\n    # Build rows in the format expected by build_feature_matrix\n    pred_rows = []\n    for _, row in user_df.iterrows():\n        pred_rows.append({\n            \"canonical_smiles\": row[\"smiles\"],\n            \"target_common_name\": row[\"target\"],\n            \"activity_class\": -1,  # unknown\n        })\n\n    X_pred, _, _ = build_feature_matrix(pred_rows, selected_columns=sel_cols)\n\n    # Predict\n    probs = model.predict_proba(X_pred)\n    preds = model.predict(X_pred)\n\n    # Applicability domain\n    dists = nn_model.kneighbors(X_pred)[0].mean(axis=1)\n    in_domain = dists <= ad_thresh\n\n    # Conformal prediction sets\n    conformal_sets = probs >= (1.0 - conf_q)\n\n    # Build output DataFrame\n    results = user_df[[\"compound_id\", \"smiles\", \"target\"]].copy()\n    # Copy extra metadata columns (compound_name, known_activity, etc.)\n    for col in extra_cols:\n        results[col] = user_df[col].values\n    for c in range(n_cls):\n        results[f\"prob_{class_map[c]}\"] = probs[:, c]\n    results[\"predicted_class\"] = preds\n    results[\"predicted_label\"] = [class_map.get(int(p), \"unknown\") for p in preds]\n    results[\"max_confidence\"] = probs.max(axis=1)\n    results[\"conformal_set\"] = [\n        \"{\"+\", \".join(class_map[c] for c in range(n_cls) if cs[c])+\"}\"\n        for cs in conformal_sets\n    ]\n    results[\"in_domain\"] = in_domain\n    results[\"knn_distance\"] = dists\n    results[\"valid_smiles\"] = user_df[\"valid_smiles\"]\n\n    return results\n\n\n# ── Run predictions ───────────────────────────────────────────────────\nif PREDICTION_CSV.exists():\n    results = predict_compounds(PREDICTION_CSV, model_path)\n\n    # Save\n    out_path = OUTPUT_DIR / \"predictions.csv\"\n    results.to_csv(out_path, index=False)\n    print(f\"Predictions saved to: {out_path}\")\n    print(f\"Compounds predicted: {len(results)}\")\n    print(f\"In-domain: {results['in_domain'].sum()}/{len(results)}\")\n    print()\n    display(results.style.format({\n        \"prob_inactive\": \"{:.4f}\",\n        \"prob_less_potent\": \"{:.4f}\",\n        \"prob_potent\": \"{:.4f}\",\n        \"max_confidence\": \"{:.4f}\",\n        \"knn_distance\": \"{:.4f}\",\n    }).set_caption(\"Predictions\"))\nelse:\n    print(f\"No prediction file found at: {PREDICTION_CSV}\")\n    print(\"Create a CSV with columns: compound_id, smiles, target\")\n    print(f\"and place it at {PREDICTION_CSV}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Figure 8: Prediction summary visualization ───────────────────────\n",
    "if PREDICTION_CSV.exists():\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Class distribution of predictions\n",
    "    pred_counts = results[\"predicted_label\"].value_counts()\n",
    "    label_order = [\"inactive\", \"less_potent\", \"potent\"]\n",
    "    pred_counts = pred_counts.reindex(label_order, fill_value=0)\n",
    "    bars = axes[0].bar(\n",
    "        pred_counts.index, pred_counts.values,\n",
    "        color=[CLASS_COLORS[i] for i in range(NUM_CLASSES)],\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    for bar, val in zip(bars, pred_counts.values):\n",
    "        axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2,\n",
    "                     str(val), ha=\"center\", fontweight=\"bold\")\n",
    "    axes[0].set_title(\"Predicted Class Distribution\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    # Confidence distribution\n",
    "    axes[1].hist(results[\"max_confidence\"], bins=20, color=\"#3498db\",\n",
    "                 edgecolor=\"black\", alpha=0.8)\n",
    "    axes[1].axvline(0.5, color=\"red\", ls=\"--\", lw=1.5, label=\"50% threshold\")\n",
    "    axes[1].set_xlabel(\"Max Class Probability\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "    axes[1].set_title(\"Prediction Confidence\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Domain coverage\n",
    "    domain_counts = results[\"in_domain\"].value_counts()\n",
    "    axes[2].pie(\n",
    "        [domain_counts.get(True, 0), domain_counts.get(False, 0)],\n",
    "        labels=[\"In Domain\", \"Out of Domain\"],\n",
    "        colors=[\"#2ecc71\", \"#e74c3c\"],\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        textprops={\"fontsize\": 12},\n",
    "    )\n",
    "    axes[2].set_title(\"Applicability Domain\")\n",
    "\n",
    "    fig.suptitle(\"Prediction Summary\", fontsize=14, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUTPUT_DIR / \"08_prediction_summary.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Load a Saved Model (Standalone Prediction)\n",
    "\n",
    "If you already trained and saved a model (Section 8), you can skip training entirely\n",
    "and jump straight to predictions. Just run the cell below with your CSV path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standalone prediction from saved model ────────────────────────────\n",
    "# Set YOUR_CSV below and run this cell.\n",
    "# No training cells need to be run first.\n",
    "\n",
    "YOUR_CSV = NOTEBOOK_DIR / \"data\" / \"example_predictions.csv\"  # <-- change this\n",
    "SAVED_MODEL = MODEL_DIR / \"safety_model.pkl\"\n",
    "\n",
    "if SAVED_MODEL.exists() and YOUR_CSV.exists():\n",
    "    standalone_results = predict_compounds(YOUR_CSV, SAVED_MODEL)\n",
    "    standalone_results.to_csv(OUTPUT_DIR / \"standalone_predictions.csv\", index=False)\n",
    "    print(f\"Predictions saved to: {OUTPUT_DIR / 'standalone_predictions.csv'}\")\n",
    "    display(standalone_results)\n",
    "elif not SAVED_MODEL.exists():\n",
    "    print(f\"No saved model found at {SAVED_MODEL}. Run training cells first.\")\n",
    "else:\n",
    "    print(f\"No input CSV found at {YOUR_CSV}. Update the YOUR_CSV variable above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}